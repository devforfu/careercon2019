{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT = Path.cwd().parent/'input'\n",
    "ROOT = Path.home()/'data'/'careercon2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = ROOT/'sample_submission.csv'\n",
    "TRAIN = ROOT/'X_train.csv'\n",
    "TARGET = ROOT/'y_train.csv'\n",
    "TEST = ROOT/'X_test.csv'\n",
    "\n",
    "ID_COLS = ['series_id', 'measurement_number']\n",
    "\n",
    "x_cols = {\n",
    "    'series_id': np.uint32,\n",
    "    'measurement_number': np.uint32,\n",
    "    'orientation_X': np.float32,\n",
    "    'orientation_Y': np.float32,\n",
    "    'orientation_Z': np.float32,\n",
    "    'orientation_W': np.float32,\n",
    "    'angular_velocity_X': np.float32,\n",
    "    'angular_velocity_Y': np.float32,\n",
    "    'angular_velocity_Z': np.float32,\n",
    "    'linear_acceleration_X': np.float32,\n",
    "    'linear_acceleration_Y': np.float32,\n",
    "    'linear_acceleration_Z': np.float32\n",
    "}\n",
    "\n",
    "y_cols = {\n",
    "    'series_id': np.uint32,\n",
    "    'group_id': np.uint32,\n",
    "    'surface': str\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn = pd.read_csv(TRAIN, usecols=x_cols.keys(), dtype=x_cols)\n",
    "x_tst = pd.read_csv(TEST, usecols=x_cols.keys(), dtype=x_cols)\n",
    "y_trn = pd.read_csv(TARGET, usecols=y_cols.keys(), dtype=y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_euler_angles(df):\n",
    "    \"\"\"Adds Euler angles features to the dataset.\"\"\"\n",
    "    \n",
    "    x, y, z, w = [df[f'orientation_{s}'] for s in list('XYZW')]\n",
    "    nx, ny, nz = quaternion_to_euler(x, y, z, w)\n",
    "    df['euler_X'] = nx\n",
    "    df['euler_Y'] = ny\n",
    "    df['euler_Z'] = nz\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_to_euler(x, y, z, w):\n",
    "    \"\"\"Converts quaternion values into Euler angles (roll, pitch and yaw).\"\"\"\n",
    "    \n",
    "    t0 = 2.0*(w*x + y*z)\n",
    "    t1 = 1.0 - 2.0*(x*x + y*y)\n",
    "    X = np.arctan2(t0, t1)\n",
    "    \n",
    "    t2 = np.clip(2.0*(w*y - z*x), -1, 1)\n",
    "    Y = np.arcsin(t2)\n",
    "    \n",
    "    t3 = 2.0*(w*z + x*y)\n",
    "    t4 = 1.0 - 2.0*(y*y + z*z)\n",
    "    Z = np.arctan2(t3, t4)\n",
    "    \n",
    "    return X, Y, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startswith(df, prefix):\n",
    "    return df.columns[df.columns.str.startswith(prefix)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of series: 3810 train, 3816 test\n"
     ]
    }
   ],
   "source": [
    "trn_sz, tst_sz = x_trn.series_id.nunique(), x_tst.series_id.nunique()\n",
    "print(f'Number of series: {trn_sz} train, {tst_sz} test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tst['series_id'] += len(x_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([x_trn, x_tst], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_euler_angles(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['measurement_number'] + startswith(data, 'orient'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>243558</th>\n",
       "      <th>119111</th>\n",
       "      <th>928918</th>\n",
       "      <th>815480</th>\n",
       "      <th>249925</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>series_id</th>\n",
       "      <td>1902.000000</td>\n",
       "      <td>930.000000</td>\n",
       "      <td>491127.000000</td>\n",
       "      <td>490240.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angular_velocity_X</th>\n",
       "      <td>-0.027397</td>\n",
       "      <td>0.023170</td>\n",
       "      <td>0.129640</td>\n",
       "      <td>0.037026</td>\n",
       "      <td>-0.017861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angular_velocity_Y</th>\n",
       "      <td>-0.038235</td>\n",
       "      <td>-0.071596</td>\n",
       "      <td>-0.003962</td>\n",
       "      <td>-0.011218</td>\n",
       "      <td>-0.045124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angular_velocity_Z</th>\n",
       "      <td>0.141130</td>\n",
       "      <td>0.110860</td>\n",
       "      <td>0.074504</td>\n",
       "      <td>-0.085403</td>\n",
       "      <td>-0.091425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear_acceleration_X</th>\n",
       "      <td>0.546210</td>\n",
       "      <td>1.192700</td>\n",
       "      <td>7.679400</td>\n",
       "      <td>1.004200</td>\n",
       "      <td>-2.370200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear_acceleration_Y</th>\n",
       "      <td>1.838000</td>\n",
       "      <td>1.558500</td>\n",
       "      <td>-0.908840</td>\n",
       "      <td>2.135700</td>\n",
       "      <td>2.500600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear_acceleration_Z</th>\n",
       "      <td>-9.821200</td>\n",
       "      <td>-10.435000</td>\n",
       "      <td>-9.836400</td>\n",
       "      <td>-11.972000</td>\n",
       "      <td>-9.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euler_X</th>\n",
       "      <td>2.842510</td>\n",
       "      <td>2.837526</td>\n",
       "      <td>2.847498</td>\n",
       "      <td>2.842665</td>\n",
       "      <td>2.841162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euler_Y</th>\n",
       "      <td>-0.009827</td>\n",
       "      <td>-0.012305</td>\n",
       "      <td>-0.018179</td>\n",
       "      <td>-0.017390</td>\n",
       "      <td>-0.011367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euler_Z</th>\n",
       "      <td>-1.595382</td>\n",
       "      <td>-1.796759</td>\n",
       "      <td>1.863175</td>\n",
       "      <td>-3.101702</td>\n",
       "      <td>-0.247120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            243558      119111         928918         815480  \\\n",
       "series_id              1902.000000  930.000000  491127.000000  490240.000000   \n",
       "angular_velocity_X       -0.027397    0.023170       0.129640       0.037026   \n",
       "angular_velocity_Y       -0.038235   -0.071596      -0.003962      -0.011218   \n",
       "angular_velocity_Z        0.141130    0.110860       0.074504      -0.085403   \n",
       "linear_acceleration_X     0.546210    1.192700       7.679400       1.004200   \n",
       "linear_acceleration_Y     1.838000    1.558500      -0.908840       2.135700   \n",
       "linear_acceleration_Z    -9.821200  -10.435000      -9.836400     -11.972000   \n",
       "euler_X                   2.842510    2.837526       2.847498       2.842665   \n",
       "euler_Y                  -0.009827   -0.012305      -0.018179      -0.017390   \n",
       "euler_Z                  -1.595382   -1.796759       1.863175      -3.101702   \n",
       "\n",
       "                            249925  \n",
       "series_id              1952.000000  \n",
       "angular_velocity_X       -0.017861  \n",
       "angular_velocity_Y       -0.045124  \n",
       "angular_velocity_Z       -0.091425  \n",
       "linear_acceleration_X    -2.370200  \n",
       "linear_acceleration_Y     2.500600  \n",
       "linear_acceleration_Z    -9.029200  \n",
       "euler_X                   2.841162  \n",
       "euler_Y                  -0.011367  \n",
       "euler_Z                  -0.247120  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "euler_cols = startswith(data, 'euler')\n",
    "linear_cols = startswith(data, 'linear') \n",
    "angular_cols = startswith(data, 'angular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_fft(arr): return np.abs(np.fft.rfft(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_mean(x): return x - x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x): return (x - x.mean())/x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = data.groupby('series_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    groups[euler_cols].diff().fillna(0),\n",
    "    groups[linear_cols].transform(zero_mean),\n",
    "    groups[angular_cols].transform(zero_mean)\n",
    "], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_data = (\n",
    "    groups[linear_cols + angular_cols]\n",
    "    .apply(lambda df: df.apply(abs_fft, axis=0))\n",
    "    .reset_index('series_id', drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 128\n",
    "fft_seq_len = seq_len//2 + 1\n",
    "num_classes = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared datasets shapes: (7626, 9, 128) raw, (7626, 6, 65) fft\n"
     ]
    }
   ],
   "source": [
    "# Shape of array: (batch, features, time dimension)\n",
    "raw_arr = data.values.reshape([trn_sz + tst_sz, len(data.columns),  seq_len])\n",
    "fft_arr = fft_data.values.reshape([trn_sz + tst_sz, len(fft_data.columns), fft_seq_len])\n",
    "print(f'Prepared datasets shapes: {raw_arr.shape} raw, {fft_arr.shape} fft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_arr = np.load('/home/ck/data/careercon2019/tmp/feat.npy').transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(raw_arr, feat_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder().fit(y_trn['surface'])\n",
    "target = list(enc.transform(y_trn['surface']))\n",
    "target += [0] * tst_sz\n",
    "target = np.array(target)\n",
    "assert len(target) == trn_sz + tst_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(data, target, train_size, valid_pct=0.1, seed=None):\n",
    "    raw, fft = data\n",
    "    assert len(raw) == len(fft)\n",
    "    sz = train_size\n",
    "    idx = np.arange(sz)\n",
    "    trn_idx, val_idx = train_test_split(\n",
    "        idx, test_size=valid_pct, random_state=seed)\n",
    "    trn_ds = TensorDataset(\n",
    "        torch.tensor(raw[:sz][trn_idx]).float(), \n",
    "        torch.tensor(fft[:sz][trn_idx]).float(), \n",
    "        torch.tensor(target[:sz][trn_idx]).long())\n",
    "    val_ds = TensorDataset(\n",
    "        torch.tensor(raw[:sz][val_idx]).float(), \n",
    "        torch.tensor(fft[:sz][val_idx]).float(), \n",
    "        torch.tensor(target[:sz][val_idx]).long())\n",
    "    tst_ds = TensorDataset(\n",
    "        torch.tensor(raw[sz:]).float(), \n",
    "        torch.tensor(fft[sz:]).float(), \n",
    "        torch.tensor(target[sz:]).long())\n",
    "    return trn_ds, val_ds, tst_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(data, bs=128, jobs=0):\n",
    "    trn_ds, val_ds, tst_ds = data\n",
    "    trn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True, num_workers=jobs)\n",
    "    val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n",
    "    tst_dl = DataLoader(tst_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n",
    "    return trn_dl, val_dl, tst_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = create_datasets((raw_arr, fft_arr), target, trn_sz, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn1d_drop_layer(layers, n_outputs, drop=None, bn=True, activ=nn.ReLU):\n",
    "    \"\"\"Adds batchnorm, dropout, and/or activation layer(s) \n",
    "    to the list of layers.\n",
    "    \"\"\"\n",
    "    if bn:\n",
    "        layers += [nn.BatchNorm1d(n_outputs)]\n",
    "    if activ is not None:\n",
    "        layers += [activ()]\n",
    "    if drop and 0.0 < drop < 1.0:\n",
    "        layers += [nn.Dropout(drop)]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(ni, no, kernel=3, stride=1, pad=0,\n",
    "           drop=None, bn=True, activ=nn.ReLU):\n",
    "    \"\"\"A 1-d convolutional layer with few additional layers on top of it.\"\"\"\n",
    "    \n",
    "    layers = [nn.Conv1d(ni, no, kernel, stride, pad, bias=not bn)]\n",
    "    return bn1d_drop_layer(layers, no, drop, bn, activ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(ni, no, drop=None, bn=True, activ=nn.ReLU):\n",
    "    \"\"\"A fully connected layer with few additional layers on top of it.\"\"\"\n",
    "    \n",
    "    layers = [nn.Linear(ni, no, bias=not bn)]\n",
    "    return bn1d_drop_layer(layers, no, drop, bn, activ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv1d(nn.Module):\n",
    "    def __init__(self, ni, no):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(ni, ni, kernel_size=3, padding=1, groups=ni)\n",
    "        self.pointwise = nn.Conv1d(ni, no, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pointwise(self.depthwise(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n",
    "\n",
    "    def __init__(self, keep_batch_dim=True):\n",
    "        super().__init__()\n",
    "        self.keep_batch_dim = keep_batch_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.keep_batch_dim:\n",
    "            return x.view(x.size(0), -1)\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeSeriesEncoder(nn.Module):\n",
    "#     def __init__(self, ni, drop=.5):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList([\n",
    "#             *conv1d( ni,  32, drop=drop),\n",
    "#             *conv1d( 32,  64, drop=drop),\n",
    "#             *conv1d( 64, 128, drop=drop),\n",
    "#             *conv1d(128, 256, drop=drop),\n",
    "#             nn.AdaptiveAvgPool1d(1),\n",
    "#             Flatten(),\n",
    "#             nn.Dropout(drop),\n",
    "#             *fc(256, 64, drop=drop),\n",
    "#             *fc( 64, 64)\n",
    "#         ])\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Debugger(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeSeriesEncoder(nn.Module):\n",
    "#     def __init__(self, ni, drop=.5):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList([\n",
    "#             nn.Conv1d(ni, 32, 8, 2, padding=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(drop),\n",
    "#             nn.Conv1d(32, 64, 8, 4, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(drop),\n",
    "#             nn.Conv1d(64, 128, 8, 4, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(drop),\n",
    "#             nn.Conv1d(128, 256, 8, 4, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             Flatten(),\n",
    "#             nn.Dropout(drop),\n",
    "#             nn.Linear(256, 64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(drop),\n",
    "#             nn.Linear(64, 64),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         ])\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d(nn.Module):\n",
    "    def __init__(self, ni, no, kernel, stride=1, pad=0, drop=.5):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(ni, no, kernel, stride, padding=pad),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(drop))\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesEncoder(nn.Module):\n",
    "    def __init__(self, conv, top, drop=.5):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for ni, no, kernel, stride, pad in conv:\n",
    "            layers.append(Conv1d(ni, no, kernel, stride, pad, drop))\n",
    "            # layers += [\n",
    "            #    nn.Conv1d(ni, no, kernel, stride, padding=pad),\n",
    "            #    nn.ReLU(inplace=True),\n",
    "            #    nn.Dropout(drop)]\n",
    "        layers.append(Flatten())\n",
    "        _, in_size, *_ = conv[-1]\n",
    "        for out_size in top:\n",
    "            layers.append(nn.Dropout(drop))\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            in_size = out_size\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, raw_ni, fft_ni, no):\n",
    "        super().__init__()\n",
    "        self.raw = TimeSeriesEncoder(conv=[\n",
    "            (raw_ni,  32, 8, 2, 3),\n",
    "            (    32,  64, 8, 4, 2),\n",
    "            (    64, 128, 8, 4, 2),\n",
    "            (   128, 256, 8, 4, 2)\n",
    "        ], top=[64, 64])\n",
    "        self.fft = TimeSeriesEncoder(conv=[\n",
    "            (fft_ni,  32, 8, 2, 4),\n",
    "            (    32,  64, 8, 2, 4),\n",
    "            (    64, 128, 8, 4, 4),\n",
    "            (   128, 128, 8, 4, 4),\n",
    "            (   128, 256, 8, 2, 3)\n",
    "        ], top=[64, 64])\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, no)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t_raw, t_fft):\n",
    "        raw_out = self.raw(t_raw)\n",
    "        fft_out = self.fft(t_fft)\n",
    "        t_in = torch.cat([raw_out, fft_out], dim=1)\n",
    "        out = self.out(t_in)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + np.cos(np.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "Epoch 1 best model saved with accuracy: 21.26%\n",
      "Epoch 4 best model saved with accuracy: 23.62%\n",
      "Epoch:   5. Loss: 2.0629. Acc.: 22.57%\n",
      "Epoch 8 best model saved with accuracy: 26.77%\n",
      "Epoch:  10. Loss: 1.7151. Acc.: 28.35%\n",
      "Epoch 10 best model saved with accuracy: 28.35%\n",
      "Epoch 12 best model saved with accuracy: 29.66%\n",
      "Epoch 13 best model saved with accuracy: 31.23%\n",
      "Epoch:  15. Loss: 1.6958. Acc.: 32.81%\n",
      "Epoch 15 best model saved with accuracy: 32.81%\n",
      "Epoch 19 best model saved with accuracy: 34.38%\n",
      "Epoch:  20. Loss: 1.5414. Acc.: 32.55%\n",
      "Epoch:  25. Loss: 1.5464. Acc.: 29.66%\n",
      "Epoch:  30. Loss: 1.6755. Acc.: 31.76%\n",
      "Epoch:  35. Loss: 1.6121. Acc.: 27.56%\n",
      "Epoch:  40. Loss: 1.5160. Acc.: 28.08%\n",
      "Epoch:  45. Loss: 1.4838. Acc.: 29.40%\n",
      "Epoch:  50. Loss: 1.5336. Acc.: 29.92%\n",
      "Epoch:  55. Loss: 1.5008. Acc.: 29.13%\n",
      "Epoch:  60. Loss: 1.4944. Acc.: 30.71%\n",
      "Epoch:  65. Loss: 1.3241. Acc.: 30.97%\n",
      "Epoch:  70. Loss: 1.4255. Acc.: 30.45%\n",
      "Epoch:  75. Loss: 1.5957. Acc.: 35.43%\n",
      "Epoch 75 best model saved with accuracy: 35.43%\n",
      "Epoch:  80. Loss: 1.2854. Acc.: 29.92%\n",
      "Epoch:  85. Loss: 1.3848. Acc.: 31.23%\n",
      "Epoch:  90. Loss: 1.3755. Acc.: 32.55%\n",
      "Epoch:  95. Loss: 1.4227. Acc.: 33.86%\n",
      "Epoch: 100. Loss: 1.3674. Acc.: 31.23%\n",
      "Epoch: 105. Loss: 1.1427. Acc.: 26.77%\n",
      "Epoch 109 best model saved with accuracy: 36.48%\n",
      "Epoch: 110. Loss: 1.3259. Acc.: 28.08%\n",
      "Epoch: 115. Loss: 1.2255. Acc.: 28.87%\n",
      "Epoch: 120. Loss: 1.3086. Acc.: 32.02%\n",
      "Epoch: 125. Loss: 1.4129. Acc.: 33.07%\n",
      "Epoch: 130. Loss: 1.2881. Acc.: 32.28%\n",
      "Epoch: 135. Loss: 1.0410. Acc.: 32.55%\n",
      "Epoch: 140. Loss: 1.1892. Acc.: 33.86%\n",
      "Epoch: 145. Loss: 1.2042. Acc.: 32.81%\n",
      "Epoch: 150. Loss: 1.1129. Acc.: 33.86%\n",
      "Epoch: 155. Loss: 1.0684. Acc.: 33.33%\n",
      "Epoch: 160. Loss: 1.2056. Acc.: 33.07%\n",
      "Epoch: 165. Loss: 1.1593. Acc.: 31.76%\n",
      "Epoch: 170. Loss: 1.0360. Acc.: 33.60%\n",
      "Epoch: 175. Loss: 1.0785. Acc.: 32.81%\n",
      "Epoch: 180. Loss: 1.1106. Acc.: 30.71%\n",
      "Epoch: 185. Loss: 0.9499. Acc.: 33.33%\n",
      "Epoch: 190. Loss: 1.1686. Acc.: 32.02%\n",
      "Epoch: 195. Loss: 1.0389. Acc.: 34.12%\n",
      "Epoch: 200. Loss: 1.1322. Acc.: 34.12%\n",
      "Epoch: 205. Loss: 1.2093. Acc.: 31.23%\n",
      "Epoch: 210. Loss: 0.9574. Acc.: 34.38%\n",
      "Epoch: 215. Loss: 1.0888. Acc.: 32.28%\n",
      "Epoch: 220. Loss: 0.9743. Acc.: 35.70%\n",
      "Epoch: 225. Loss: 1.0516. Acc.: 37.80%\n",
      "Epoch 225 best model saved with accuracy: 37.80%\n",
      "Epoch: 230. Loss: 1.0122. Acc.: 34.65%\n",
      "Epoch: 235. Loss: 1.1294. Acc.: 32.02%\n",
      "Epoch: 240. Loss: 1.0034. Acc.: 35.43%\n",
      "Epoch: 245. Loss: 1.1254. Acc.: 37.01%\n",
      "Epoch: 250. Loss: 0.9429. Acc.: 34.91%\n",
      "Epoch 254 best model saved with accuracy: 38.06%\n",
      "Epoch: 255. Loss: 1.0442. Acc.: 34.91%\n",
      "Epoch: 260. Loss: 1.0604. Acc.: 37.27%\n",
      "Epoch 262 best model saved with accuracy: 38.58%\n",
      "Epoch: 265. Loss: 1.0894. Acc.: 38.58%\n",
      "Epoch: 270. Loss: 0.9492. Acc.: 36.48%\n",
      "Epoch 273 best model saved with accuracy: 39.37%\n",
      "Epoch: 275. Loss: 1.1164. Acc.: 38.85%\n",
      "Epoch: 280. Loss: 0.9759. Acc.: 36.75%\n",
      "Epoch: 285. Loss: 0.9545. Acc.: 35.17%\n",
      "Epoch: 290. Loss: 0.9967. Acc.: 37.01%\n",
      "Epoch 294 best model saved with accuracy: 40.42%\n",
      "Epoch: 295. Loss: 1.0267. Acc.: 38.32%\n",
      "Epoch: 300. Loss: 0.9691. Acc.: 36.75%\n",
      "Epoch: 305. Loss: 0.9820. Acc.: 37.53%\n",
      "Epoch: 310. Loss: 0.9696. Acc.: 40.94%\n",
      "Epoch 310 best model saved with accuracy: 40.94%\n",
      "Epoch: 315. Loss: 0.9282. Acc.: 35.43%\n",
      "Epoch: 320. Loss: 0.8727. Acc.: 35.70%\n",
      "Epoch: 325. Loss: 1.0953. Acc.: 41.99%\n",
      "Epoch 325 best model saved with accuracy: 41.99%\n",
      "Epoch: 330. Loss: 0.9787. Acc.: 37.80%\n",
      "Epoch: 335. Loss: 1.0606. Acc.: 35.17%\n",
      "Epoch: 340. Loss: 0.7909. Acc.: 35.43%\n",
      "Epoch: 345. Loss: 1.1160. Acc.: 37.80%\n",
      "Epoch: 350. Loss: 0.9021. Acc.: 38.85%\n",
      "Epoch: 355. Loss: 0.8705. Acc.: 34.12%\n",
      "Epoch: 360. Loss: 0.9743. Acc.: 36.75%\n",
      "Epoch: 365. Loss: 0.9119. Acc.: 38.06%\n",
      "Epoch: 370. Loss: 0.7935. Acc.: 36.75%\n",
      "Epoch: 375. Loss: 0.9487. Acc.: 37.27%\n",
      "Epoch: 380. Loss: 0.7507. Acc.: 39.63%\n",
      "Epoch: 385. Loss: 0.7925. Acc.: 41.47%\n",
      "Epoch: 390. Loss: 1.0590. Acc.: 37.80%\n",
      "Epoch: 395. Loss: 0.9335. Acc.: 36.75%\n",
      "Epoch: 400. Loss: 0.7616. Acc.: 37.80%\n",
      "Epoch: 405. Loss: 0.7577. Acc.: 37.27%\n",
      "Epoch: 410. Loss: 1.0468. Acc.: 38.06%\n",
      "Epoch: 415. Loss: 0.9383. Acc.: 39.11%\n",
      "Epoch: 420. Loss: 1.0800. Acc.: 39.90%\n",
      "Epoch: 425. Loss: 0.9351. Acc.: 36.48%\n",
      "Epoch: 430. Loss: 0.7873. Acc.: 36.75%\n",
      "Epoch: 435. Loss: 0.9534. Acc.: 37.80%\n",
      "Epoch: 440. Loss: 0.8149. Acc.: 38.32%\n",
      "Epoch: 445. Loss: 0.9794. Acc.: 38.32%\n",
      "Epoch: 450. Loss: 0.8337. Acc.: 37.80%\n",
      "Epoch: 455. Loss: 0.8054. Acc.: 37.80%\n",
      "Epoch: 460. Loss: 1.1441. Acc.: 36.48%\n",
      "Epoch: 465. Loss: 0.9482. Acc.: 37.53%\n",
      "Epoch: 470. Loss: 0.8833. Acc.: 38.06%\n",
      "Epoch: 475. Loss: 0.9014. Acc.: 38.85%\n",
      "Epoch: 480. Loss: 0.8822. Acc.: 39.90%\n",
      "Epoch: 485. Loss: 0.9466. Acc.: 38.85%\n",
      "Epoch: 490. Loss: 0.9163. Acc.: 37.27%\n",
      "Epoch: 495. Loss: 0.7245. Acc.: 39.11%\n",
      "Epoch: 500. Loss: 0.8899. Acc.: 36.75%\n",
      "Epoch: 505. Loss: 0.7833. Acc.: 40.68%\n",
      "Epoch: 510. Loss: 0.8957. Acc.: 37.53%\n",
      "Epoch: 515. Loss: 0.9501. Acc.: 40.94%\n",
      "Epoch: 520. Loss: 0.7535. Acc.: 36.22%\n",
      "Epoch: 525. Loss: 0.8000. Acc.: 38.06%\n",
      "Epoch: 530. Loss: 0.9984. Acc.: 37.80%\n",
      "Epoch: 535. Loss: 0.9647. Acc.: 38.85%\n",
      "Epoch: 540. Loss: 0.6124. Acc.: 39.37%\n",
      "Epoch: 545. Loss: 0.9939. Acc.: 38.58%\n",
      "Epoch: 550. Loss: 0.9329. Acc.: 39.63%\n",
      "Epoch: 555. Loss: 0.9540. Acc.: 39.63%\n",
      "Epoch: 560. Loss: 0.8442. Acc.: 40.16%\n",
      "Epoch: 565. Loss: 0.8239. Acc.: 37.27%\n",
      "Epoch: 570. Loss: 0.6465. Acc.: 38.32%\n",
      "Epoch: 575. Loss: 1.0174. Acc.: 37.53%\n",
      "Epoch: 580. Loss: 0.8572. Acc.: 37.01%\n",
      "Epoch: 585. Loss: 1.1038. Acc.: 37.80%\n",
      "Epoch: 590. Loss: 1.0072. Acc.: 39.90%\n",
      "Epoch: 595. Loss: 0.8909. Acc.: 38.58%\n",
      "Epoch: 600. Loss: 0.8235. Acc.: 38.32%\n",
      "Epoch: 605. Loss: 0.7901. Acc.: 37.01%\n",
      "Epoch: 610. Loss: 0.7066. Acc.: 38.85%\n",
      "Epoch: 615. Loss: 0.6564. Acc.: 38.58%\n",
      "Epoch: 620. Loss: 0.8425. Acc.: 37.53%\n",
      "Epoch: 625. Loss: 0.7909. Acc.: 34.91%\n",
      "Epoch: 630. Loss: 0.7548. Acc.: 39.63%\n",
      "Epoch: 635. Loss: 0.8160. Acc.: 40.16%\n",
      "Epoch: 640. Loss: 0.9502. Acc.: 40.42%\n",
      "Epoch: 645. Loss: 0.8566. Acc.: 39.37%\n",
      "Epoch: 650. Loss: 0.9203. Acc.: 38.06%\n",
      "Epoch: 655. Loss: 0.8523. Acc.: 39.11%\n",
      "Epoch: 660. Loss: 0.6095. Acc.: 40.68%\n",
      "Epoch: 665. Loss: 0.5887. Acc.: 37.53%\n",
      "Epoch: 670. Loss: 0.9282. Acc.: 38.32%\n",
      "Epoch: 675. Loss: 0.9242. Acc.: 41.73%\n",
      "Epoch: 680. Loss: 0.8471. Acc.: 39.63%\n",
      "Epoch 683 best model saved with accuracy: 43.04%\n",
      "Epoch: 685. Loss: 0.7113. Acc.: 39.11%\n",
      "Epoch: 690. Loss: 0.8489. Acc.: 39.37%\n",
      "Epoch: 695. Loss: 0.9766. Acc.: 42.52%\n",
      "Epoch: 700. Loss: 0.6267. Acc.: 41.73%\n",
      "Epoch 704 best model saved with accuracy: 43.57%\n",
      "Epoch: 705. Loss: 0.8273. Acc.: 41.21%\n",
      "Epoch: 710. Loss: 0.7929. Acc.: 41.99%\n",
      "Epoch: 715. Loss: 0.6976. Acc.: 39.90%\n",
      "Epoch 719 best model saved with accuracy: 44.09%\n",
      "Epoch: 720. Loss: 0.8331. Acc.: 40.68%\n",
      "Epoch: 725. Loss: 0.7966. Acc.: 40.16%\n",
      "Epoch: 730. Loss: 0.7285. Acc.: 39.90%\n",
      "Epoch: 735. Loss: 0.7612. Acc.: 41.99%\n",
      "Epoch: 740. Loss: 0.7983. Acc.: 38.85%\n",
      "Epoch: 745. Loss: 0.6939. Acc.: 39.63%\n",
      "Epoch: 750. Loss: 0.7683. Acc.: 40.94%\n",
      "Epoch: 755. Loss: 0.7694. Acc.: 39.37%\n",
      "Epoch: 760. Loss: 0.7841. Acc.: 39.90%\n",
      "Epoch: 765. Loss: 0.5803. Acc.: 39.90%\n",
      "Epoch: 770. Loss: 0.8634. Acc.: 37.80%\n",
      "Epoch: 775. Loss: 0.6665. Acc.: 40.68%\n",
      "Epoch: 780. Loss: 0.7230. Acc.: 40.68%\n",
      "Epoch: 785. Loss: 0.8782. Acc.: 40.94%\n",
      "Epoch: 790. Loss: 0.7011. Acc.: 38.58%\n",
      "Epoch: 795. Loss: 0.8372. Acc.: 38.32%\n",
      "Epoch: 800. Loss: 0.7653. Acc.: 39.63%\n",
      "Epoch: 805. Loss: 0.6430. Acc.: 37.80%\n",
      "Epoch: 810. Loss: 0.8403. Acc.: 38.85%\n",
      "Epoch: 815. Loss: 0.8276. Acc.: 36.75%\n",
      "Epoch: 820. Loss: 0.7975. Acc.: 39.90%\n",
      "Epoch: 825. Loss: 0.6678. Acc.: 38.58%\n",
      "Epoch: 830. Loss: 0.6313. Acc.: 40.42%\n",
      "Epoch: 835. Loss: 0.7846. Acc.: 38.58%\n",
      "Epoch: 840. Loss: 0.6336. Acc.: 38.32%\n",
      "Epoch: 845. Loss: 0.7021. Acc.: 39.11%\n",
      "Epoch: 850. Loss: 0.6546. Acc.: 38.85%\n",
      "Epoch: 855. Loss: 0.8105. Acc.: 39.63%\n",
      "Epoch: 860. Loss: 0.6769. Acc.: 38.06%\n",
      "Epoch: 865. Loss: 0.6822. Acc.: 38.85%\n",
      "Epoch: 870. Loss: 0.6494. Acc.: 40.16%\n",
      "Epoch: 875. Loss: 0.8570. Acc.: 37.80%\n",
      "Epoch: 880. Loss: 0.7836. Acc.: 39.37%\n",
      "Epoch: 885. Loss: 0.7562. Acc.: 39.11%\n",
      "Epoch: 890. Loss: 0.6953. Acc.: 41.47%\n",
      "Epoch: 895. Loss: 0.7935. Acc.: 36.22%\n",
      "Epoch: 900. Loss: 0.8704. Acc.: 40.68%\n",
      "Epoch: 905. Loss: 0.7204. Acc.: 40.42%\n",
      "Epoch: 910. Loss: 0.7840. Acc.: 39.63%\n",
      "Epoch: 915. Loss: 0.6337. Acc.: 40.16%\n",
      "Epoch: 920. Loss: 0.7064. Acc.: 40.16%\n",
      "Epoch: 925. Loss: 0.8194. Acc.: 39.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 930. Loss: 0.7082. Acc.: 39.11%\n",
      "Epoch: 935. Loss: 0.7448. Acc.: 39.63%\n",
      "Epoch: 940. Loss: 0.6917. Acc.: 42.78%\n",
      "Epoch: 945. Loss: 0.7073. Acc.: 42.26%\n",
      "Epoch: 950. Loss: 0.7643. Acc.: 37.80%\n",
      "Epoch: 955. Loss: 0.8582. Acc.: 39.90%\n",
      "Epoch: 960. Loss: 0.5466. Acc.: 40.42%\n",
      "Epoch: 965. Loss: 0.7807. Acc.: 38.06%\n",
      "Epoch: 970. Loss: 0.5902. Acc.: 37.27%\n",
      "Epoch: 975. Loss: 0.6724. Acc.: 37.53%\n",
      "Epoch: 980. Loss: 0.6606. Acc.: 39.37%\n",
      "Epoch: 985. Loss: 0.6720. Acc.: 39.37%\n",
      "Epoch: 990. Loss: 0.7509. Acc.: 41.21%\n",
      "Epoch: 995. Loss: 0.7154. Acc.: 41.73%\n",
      "Epoch: 1000. Loss: 0.6033. Acc.: 38.06%\n",
      "Epoch: 1005. Loss: 0.6504. Acc.: 38.85%\n",
      "Epoch: 1010. Loss: 0.6947. Acc.: 38.58%\n",
      "Epoch: 1015. Loss: 0.6506. Acc.: 35.96%\n",
      "Epoch: 1020. Loss: 0.7027. Acc.: 37.80%\n",
      "Epoch: 1025. Loss: 0.7430. Acc.: 38.06%\n",
      "Epoch: 1030. Loss: 0.7173. Acc.: 39.11%\n",
      "Epoch: 1035. Loss: 0.8282. Acc.: 41.73%\n",
      "Epoch: 1040. Loss: 0.6078. Acc.: 39.11%\n",
      "Epoch: 1045. Loss: 0.6722. Acc.: 40.42%\n",
      "Epoch: 1050. Loss: 0.7703. Acc.: 38.58%\n",
      "Epoch: 1055. Loss: 0.7576. Acc.: 40.16%\n",
      "Epoch: 1060. Loss: 0.7133. Acc.: 39.37%\n",
      "Epoch: 1065. Loss: 0.9000. Acc.: 40.68%\n",
      "Epoch: 1070. Loss: 0.6613. Acc.: 39.90%\n",
      "Epoch: 1075. Loss: 0.7517. Acc.: 38.85%\n",
      "Epoch: 1080. Loss: 0.7298. Acc.: 39.37%\n",
      "Epoch: 1085. Loss: 0.6672. Acc.: 40.94%\n",
      "Epoch: 1090. Loss: 0.7648. Acc.: 39.37%\n",
      "Epoch: 1095. Loss: 0.8136. Acc.: 39.11%\n",
      "Epoch: 1100. Loss: 0.7089. Acc.: 39.63%\n",
      "Epoch: 1105. Loss: 0.6700. Acc.: 39.37%\n",
      "Epoch: 1110. Loss: 0.7373. Acc.: 39.11%\n",
      "Epoch: 1115. Loss: 0.6646. Acc.: 39.90%\n",
      "Epoch: 1120. Loss: 0.7177. Acc.: 40.42%\n",
      "Epoch: 1125. Loss: 0.5463. Acc.: 40.68%\n",
      "Epoch: 1130. Loss: 0.6907. Acc.: 40.16%\n",
      "Epoch: 1135. Loss: 0.5887. Acc.: 41.47%\n",
      "Epoch: 1140. Loss: 0.7309. Acc.: 39.63%\n",
      "Epoch: 1145. Loss: 0.7142. Acc.: 41.99%\n",
      "Epoch: 1150. Loss: 0.6825. Acc.: 38.32%\n",
      "Epoch: 1155. Loss: 0.6018. Acc.: 38.58%\n",
      "Epoch: 1160. Loss: 0.6597. Acc.: 38.85%\n",
      "Epoch: 1165. Loss: 0.7624. Acc.: 38.58%\n",
      "Epoch: 1170. Loss: 0.7529. Acc.: 38.58%\n",
      "Epoch: 1175. Loss: 0.7267. Acc.: 40.68%\n",
      "Epoch: 1180. Loss: 0.6675. Acc.: 39.11%\n",
      "Epoch: 1185. Loss: 0.8362. Acc.: 39.11%\n",
      "Epoch: 1190. Loss: 0.6048. Acc.: 39.63%\n",
      "Epoch: 1195. Loss: 0.6850. Acc.: 37.80%\n",
      "Epoch: 1200. Loss: 0.6279. Acc.: 41.73%\n",
      "Epoch: 1205. Loss: 0.7352. Acc.: 40.68%\n",
      "Epoch: 1210. Loss: 0.6581. Acc.: 39.37%\n",
      "Epoch: 1215. Loss: 0.7627. Acc.: 38.85%\n",
      "Early stopping on epoch 1219\n"
     ]
    }
   ],
   "source": [
    "raw_feat = raw_arr.shape[1]\n",
    "fft_feat = fft_arr.shape[1]\n",
    "\n",
    "trn_dl, val_dl, tst_dl = create_loaders(datasets, bs=256)\n",
    "\n",
    "lr = 0.001\n",
    "n_epochs = 3000\n",
    "iterations_per_epoch = len(trn_dl)\n",
    "best_acc = 0\n",
    "patience, trials = 500, 0\n",
    "\n",
    "model = Classifier(raw_feat, fft_feat, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "sched = CyclicLR(opt, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/100))\n",
    "\n",
    "print('Start model training')\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    model.train()\n",
    "    for i, batch in enumerate(trn_dl):\n",
    "        x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n",
    "        sched.step()\n",
    "        opt.zero_grad()\n",
    "        out = model(x_raw, x_fft)\n",
    "        loss = criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for batch in val_dl:\n",
    "        x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n",
    "        out = model(x_raw, x_fft)\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "    \n",
    "    acc = correct / total\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch: {epoch:3d}. Loss: {loss.item():.4f}. Acc.: {acc:2.2%}')\n",
    "\n",
    "    if acc > best_acc:\n",
    "        trials = 0\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), 'best.pth')\n",
    "        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
