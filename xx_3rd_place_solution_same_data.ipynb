{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prepared datasets shapes:\n",
      "(7626, 9, 128) raw\n",
      "(7626, 6, 65) fft\n",
      "(7626,) target\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA = Path.home()/'data'/'careercon2019'/'tmp'\n",
    "raw_arr = np.load(DATA/'feat.npy').transpose(0, 2, 1)\n",
    "fft_arr = np.load(DATA/'feat_fft.npy').transpose(0, 2, 1)\n",
    "target = np.load(DATA/'target.npy')\n",
    "print(f'''\n",
    "Prepared datasets shapes:\n",
    "{raw_arr.shape} raw\n",
    "{fft_arr.shape} fft\n",
    "{target.shape} target\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of array: (batch, features, time dimension)\n",
    "# raw_arr = data.values.reshape([trn_sz + tst_sz, len(data.columns),  seq_len])\n",
    "# fft_arr = fft_data.values.reshape([trn_sz + tst_sz, len(fft_data.columns), fft_seq_len])\n",
    "# print(f'Prepared datasets shapes: {raw_arr.shape} raw, {fft_arr.shape} fft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = LabelEncoder().fit(y_trn['surface'])\n",
    "# target = list(enc.transform(y_trn['surface']))\n",
    "# target += [0] * tst_sz\n",
    "# target = np.array(target)\n",
    "# assert len(target) == trn_sz + tst_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(data, target, train_size, valid_pct=0.1, seed=None):\n",
    "    raw, fft = data\n",
    "    assert len(raw) == len(fft)\n",
    "    sz = train_size\n",
    "    idx = np.arange(sz)\n",
    "    trn_idx, val_idx = train_test_split(\n",
    "        idx, test_size=valid_pct, random_state=seed)\n",
    "    trn_ds = TensorDataset(\n",
    "        torch.tensor(raw[:sz][trn_idx]).float(), \n",
    "        torch.tensor(fft[:sz][trn_idx]).float(), \n",
    "        torch.tensor(target[:sz][trn_idx]).long())\n",
    "    val_ds = TensorDataset(\n",
    "        torch.tensor(raw[:sz][val_idx]).float(), \n",
    "        torch.tensor(fft[:sz][val_idx]).float(), \n",
    "        torch.tensor(target[:sz][val_idx]).long())\n",
    "    tst_ds = TensorDataset(\n",
    "        torch.tensor(raw[sz:]).float(), \n",
    "        torch.tensor(fft[sz:]).float(), \n",
    "        torch.tensor(target[sz:]).long())\n",
    "    return trn_ds, val_ds, tst_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(data, bs=128, jobs=0):\n",
    "    trn_ds, val_ds, tst_ds = data\n",
    "    trn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True, num_workers=jobs)\n",
    "    val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n",
    "    tst_dl = DataLoader(tst_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n",
    "    return trn_dl, val_dl, tst_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_sz = 3810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = create_datasets((raw_arr, fft_arr), target, trn_sz, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn1d_drop_layer(layers, n_outputs, drop=None, bn=True, activ=nn.ReLU):\n",
    "    \"\"\"Adds batchnorm, dropout, and/or activation layer(s) \n",
    "    to the list of layers.\n",
    "    \"\"\"\n",
    "    if bn:\n",
    "        layers += [nn.BatchNorm1d(n_outputs)]\n",
    "    if activ is not None:\n",
    "        layers += [activ()]\n",
    "    if drop and 0.0 < drop < 1.0:\n",
    "        layers += [nn.Dropout(drop)]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(ni, no, kernel=3, stride=1, pad=0,\n",
    "           drop=None, bn=True, activ=nn.ReLU):\n",
    "    \"\"\"A 1-d convolutional layer with few additional layers on top of it.\"\"\"\n",
    "    \n",
    "    layers = [nn.Conv1d(ni, no, kernel, stride, pad, bias=not bn)]\n",
    "    return bn1d_drop_layer(layers, no, drop, bn, activ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(ni, no, drop=None, bn=True, activ=nn.ReLU):\n",
    "    \"\"\"A fully connected layer with few additional layers on top of it.\"\"\"\n",
    "    \n",
    "    layers = [nn.Linear(ni, no, bias=not bn)]\n",
    "    return bn1d_drop_layer(layers, no, drop, bn, activ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv1d(nn.Module):\n",
    "    def __init__(self, ni, no, kernel, stride, pad):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(ni, ni, kernel, stride, padding=pad, groups=ni)\n",
    "        self.pointwise = nn.Conv1d(ni, no, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pointwise(self.depthwise(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n",
    "\n",
    "    def __init__(self, keep_batch_dim=True):\n",
    "        super().__init__()\n",
    "        self.keep_batch_dim = keep_batch_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.keep_batch_dim:\n",
    "            return x.view(x.size(0), -1)\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, raw_ni, fft_ni, no):\n",
    "        super().__init__()\n",
    "        # 3, 2, 2, 2\n",
    "        self.raw = nn.Sequential(\n",
    "            SeparableConv1d(raw_ni, 32, 8, 2, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.5),\n",
    "            SeparableConv1d(32, 64, 8, 4, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.5),\n",
    "            SeparableConv1d(64, 128, 8, 4, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.5),\n",
    "            SeparableConv1d(128, 256, 8, 4, 2),\n",
    "            nn.ReLU(),\n",
    "            Flatten(),\n",
    "            nn.Dropout(.5),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.5),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 4, 4, 4, 4, 3\n",
    "        self.fft = nn.Sequential(\n",
    "            SeparableConv1d(fft_ni, 32, 8, 2, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.5),\n",
    "            SeparableConv1d(32, 64, 8, 2, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.5),\n",
    "            SeparableConv1d(64, 128, 8, 4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.5),\n",
    "            SeparableConv1d(128, 128, 8, 4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.5),\n",
    "            SeparableConv1d(128, 256, 8, 2, 3),\n",
    "            nn.ReLU(),\n",
    "            Flatten(),\n",
    "            nn.Dropout(.5),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.5),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, no)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t_raw, t_fft):\n",
    "        raw_out = self.raw(t_raw)\n",
    "        fft_out = self.fft(t_fft)\n",
    "        t_in = torch.cat([raw_out, fft_out], dim=1)\n",
    "        out = self.out(t_in)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + np.cos(np.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "Epoch 1 best model saved with accuracy: 16.54%\n",
      "Epoch 2 best model saved with accuracy: 19.69%\n",
      "Epoch:   5. Loss: 2.0481. Acc.: 19.69%\n",
      "Epoch:  10. Loss: 2.0159. Acc.: 19.69%\n",
      "Epoch:  15. Loss: 1.8437. Acc.: 33.07%\n",
      "Epoch 15 best model saved with accuracy: 33.07%\n",
      "Epoch 18 best model saved with accuracy: 34.12%\n",
      "Epoch:  20. Loss: 1.6236. Acc.: 35.17%\n",
      "Epoch 20 best model saved with accuracy: 35.17%\n",
      "Epoch 23 best model saved with accuracy: 35.70%\n",
      "Epoch 24 best model saved with accuracy: 38.58%\n",
      "Epoch:  25. Loss: 1.6183. Acc.: 38.06%\n",
      "Epoch 26 best model saved with accuracy: 40.42%\n",
      "Epoch 28 best model saved with accuracy: 40.68%\n",
      "Epoch:  30. Loss: 1.3745. Acc.: 45.93%\n",
      "Epoch 30 best model saved with accuracy: 45.93%\n",
      "Epoch 31 best model saved with accuracy: 46.98%\n",
      "Epoch 32 best model saved with accuracy: 50.66%\n",
      "Epoch 33 best model saved with accuracy: 56.43%\n",
      "Epoch:  35. Loss: 1.1143. Acc.: 54.86%\n",
      "Epoch 36 best model saved with accuracy: 58.53%\n",
      "Epoch 38 best model saved with accuracy: 62.73%\n",
      "Epoch 39 best model saved with accuracy: 63.25%\n",
      "Epoch:  40. Loss: 1.1609. Acc.: 63.52%\n",
      "Epoch 40 best model saved with accuracy: 63.52%\n",
      "Epoch 44 best model saved with accuracy: 65.88%\n",
      "Epoch:  45. Loss: 0.9396. Acc.: 64.30%\n",
      "Epoch:  50. Loss: 0.9413. Acc.: 66.67%\n",
      "Epoch 50 best model saved with accuracy: 66.67%\n",
      "Epoch:  55. Loss: 0.9128. Acc.: 65.09%\n",
      "Epoch:  60. Loss: 0.9207. Acc.: 64.83%\n",
      "Epoch:  65. Loss: 0.8696. Acc.: 64.30%\n",
      "Epoch 69 best model saved with accuracy: 67.45%\n",
      "Epoch:  70. Loss: 1.1066. Acc.: 66.93%\n",
      "Epoch:  75. Loss: 0.8677. Acc.: 67.72%\n",
      "Epoch 75 best model saved with accuracy: 67.72%\n",
      "Epoch 78 best model saved with accuracy: 67.98%\n",
      "Epoch:  80. Loss: 0.7965. Acc.: 67.45%\n",
      "Epoch 82 best model saved with accuracy: 68.77%\n",
      "Epoch:  85. Loss: 0.9169. Acc.: 69.03%\n",
      "Epoch 85 best model saved with accuracy: 69.03%\n",
      "Epoch:  90. Loss: 1.0530. Acc.: 69.03%\n",
      "Epoch 94 best model saved with accuracy: 69.29%\n",
      "Epoch:  95. Loss: 0.8649. Acc.: 65.88%\n",
      "Epoch: 100. Loss: 0.7403. Acc.: 68.24%\n",
      "Epoch 102 best model saved with accuracy: 69.82%\n",
      "Epoch 104 best model saved with accuracy: 70.08%\n",
      "Epoch: 105. Loss: 0.8600. Acc.: 68.77%\n",
      "Epoch: 110. Loss: 0.8598. Acc.: 70.60%\n",
      "Epoch 110 best model saved with accuracy: 70.60%\n",
      "Epoch: 115. Loss: 0.7403. Acc.: 70.34%\n",
      "Epoch 117 best model saved with accuracy: 71.65%\n",
      "Epoch: 120. Loss: 0.9551. Acc.: 69.29%\n",
      "Epoch: 125. Loss: 0.8943. Acc.: 69.03%\n",
      "Epoch: 130. Loss: 0.8155. Acc.: 69.82%\n",
      "Epoch: 135. Loss: 0.7576. Acc.: 71.92%\n",
      "Epoch 135 best model saved with accuracy: 71.92%\n",
      "Epoch: 140. Loss: 0.7850. Acc.: 70.87%\n",
      "Epoch: 145. Loss: 0.7593. Acc.: 69.82%\n",
      "Epoch 149 best model saved with accuracy: 72.18%\n",
      "Epoch: 150. Loss: 0.9064. Acc.: 70.87%\n",
      "Epoch 154 best model saved with accuracy: 73.49%\n",
      "Epoch: 155. Loss: 0.8261. Acc.: 72.97%\n",
      "Epoch 158 best model saved with accuracy: 74.28%\n",
      "Epoch: 160. Loss: 0.7503. Acc.: 71.13%\n",
      "Epoch: 165. Loss: 0.8085. Acc.: 74.02%\n",
      "Epoch: 170. Loss: 0.8898. Acc.: 73.23%\n",
      "Epoch 173 best model saved with accuracy: 74.54%\n",
      "Epoch: 175. Loss: 0.6709. Acc.: 73.49%\n",
      "Epoch: 180. Loss: 0.6906. Acc.: 74.54%\n",
      "Epoch: 185. Loss: 0.8042. Acc.: 73.49%\n",
      "Epoch: 190. Loss: 0.7534. Acc.: 71.39%\n",
      "Epoch: 195. Loss: 0.5888. Acc.: 72.70%\n",
      "Epoch: 200. Loss: 0.5827. Acc.: 73.75%\n",
      "Epoch: 205. Loss: 0.7106. Acc.: 71.65%\n",
      "Epoch 206 best model saved with accuracy: 75.59%\n",
      "Epoch: 210. Loss: 0.7096. Acc.: 73.49%\n",
      "Epoch: 215. Loss: 0.7030. Acc.: 73.75%\n",
      "Epoch: 220. Loss: 0.6800. Acc.: 73.49%\n",
      "Epoch: 225. Loss: 0.7155. Acc.: 74.80%\n",
      "Epoch: 230. Loss: 0.6699. Acc.: 71.92%\n",
      "Epoch: 235. Loss: 0.7203. Acc.: 69.29%\n",
      "Epoch: 240. Loss: 0.6781. Acc.: 74.02%\n",
      "Epoch: 245. Loss: 0.8959. Acc.: 73.75%\n",
      "Epoch: 250. Loss: 0.5949. Acc.: 74.28%\n",
      "Epoch: 255. Loss: 0.7265. Acc.: 72.18%\n",
      "Epoch: 260. Loss: 0.6834. Acc.: 74.28%\n",
      "Epoch: 265. Loss: 0.8241. Acc.: 71.92%\n",
      "Epoch 267 best model saved with accuracy: 76.12%\n",
      "Epoch: 270. Loss: 0.7396. Acc.: 74.28%\n",
      "Epoch: 275. Loss: 0.6243. Acc.: 74.80%\n",
      "Epoch: 280. Loss: 0.6386. Acc.: 73.23%\n",
      "Epoch: 285. Loss: 0.6486. Acc.: 74.02%\n",
      "Epoch: 290. Loss: 0.6226. Acc.: 73.75%\n",
      "Epoch: 295. Loss: 0.6672. Acc.: 73.23%\n",
      "Epoch: 300. Loss: 0.7462. Acc.: 74.02%\n",
      "Epoch: 305. Loss: 0.9047. Acc.: 75.85%\n",
      "Epoch 308 best model saved with accuracy: 76.38%\n",
      "Epoch: 310. Loss: 0.5968. Acc.: 74.80%\n",
      "Epoch: 315. Loss: 0.6696. Acc.: 75.33%\n",
      "Epoch: 320. Loss: 0.6821. Acc.: 75.85%\n",
      "Epoch: 325. Loss: 0.5042. Acc.: 75.07%\n",
      "Epoch: 330. Loss: 0.5852. Acc.: 74.54%\n",
      "Epoch: 335. Loss: 0.6683. Acc.: 75.07%\n",
      "Epoch 339 best model saved with accuracy: 76.64%\n",
      "Epoch: 340. Loss: 0.6086. Acc.: 75.07%\n",
      "Epoch: 345. Loss: 0.5815. Acc.: 75.59%\n",
      "Epoch: 350. Loss: 0.5838. Acc.: 75.85%\n",
      "Epoch: 355. Loss: 0.5896. Acc.: 75.07%\n",
      "Epoch: 360. Loss: 0.5183. Acc.: 74.80%\n",
      "Epoch: 365. Loss: 0.6242. Acc.: 75.59%\n",
      "Epoch: 370. Loss: 0.7186. Acc.: 75.85%\n",
      "Epoch: 375. Loss: 0.5993. Acc.: 74.80%\n",
      "Epoch: 380. Loss: 0.6597. Acc.: 74.28%\n",
      "Epoch: 385. Loss: 0.7319. Acc.: 76.12%\n",
      "Epoch 386 best model saved with accuracy: 76.90%\n",
      "Epoch: 390. Loss: 0.6483. Acc.: 75.33%\n",
      "Epoch: 395. Loss: 0.5594. Acc.: 75.33%\n",
      "Epoch: 400. Loss: 0.6655. Acc.: 74.02%\n",
      "Epoch: 405. Loss: 0.6709. Acc.: 75.85%\n",
      "Epoch: 410. Loss: 0.7463. Acc.: 75.07%\n",
      "Epoch: 415. Loss: 0.8027. Acc.: 76.12%\n",
      "Epoch: 420. Loss: 0.6960. Acc.: 75.85%\n",
      "Epoch: 425. Loss: 0.5823. Acc.: 76.12%\n",
      "Epoch: 430. Loss: 0.6392. Acc.: 77.69%\n",
      "Epoch 430 best model saved with accuracy: 77.69%\n",
      "Epoch: 435. Loss: 0.5121. Acc.: 76.12%\n",
      "Epoch: 440. Loss: 0.8411. Acc.: 75.33%\n",
      "Epoch 444 best model saved with accuracy: 79.00%\n",
      "Epoch: 445. Loss: 0.4852. Acc.: 76.64%\n",
      "Epoch: 450. Loss: 0.6939. Acc.: 75.85%\n",
      "Epoch: 455. Loss: 0.4932. Acc.: 75.07%\n",
      "Epoch: 460. Loss: 0.4669. Acc.: 76.38%\n",
      "Epoch: 465. Loss: 0.6475. Acc.: 76.64%\n",
      "Epoch: 470. Loss: 0.5446. Acc.: 76.64%\n",
      "Epoch: 475. Loss: 0.5312. Acc.: 76.90%\n",
      "Epoch: 480. Loss: 0.5723. Acc.: 75.59%\n",
      "Epoch: 485. Loss: 0.5192. Acc.: 76.64%\n",
      "Epoch: 490. Loss: 0.5306. Acc.: 77.69%\n",
      "Epoch: 495. Loss: 0.7522. Acc.: 75.07%\n",
      "Epoch: 500. Loss: 0.5923. Acc.: 77.69%\n",
      "Epoch: 505. Loss: 0.5711. Acc.: 77.95%\n",
      "Epoch: 510. Loss: 0.4879. Acc.: 76.38%\n",
      "Epoch: 515. Loss: 0.5915. Acc.: 75.59%\n",
      "Epoch: 520. Loss: 0.5262. Acc.: 76.90%\n",
      "Epoch: 525. Loss: 0.5941. Acc.: 78.22%\n",
      "Epoch: 530. Loss: 0.5259. Acc.: 76.12%\n",
      "Epoch: 535. Loss: 0.5298. Acc.: 77.43%\n",
      "Epoch 538 best model saved with accuracy: 79.53%\n",
      "Epoch: 540. Loss: 0.5601. Acc.: 75.33%\n",
      "Epoch: 545. Loss: 0.5451. Acc.: 76.64%\n",
      "Epoch: 550. Loss: 0.5731. Acc.: 76.12%\n",
      "Epoch: 555. Loss: 0.6427. Acc.: 76.64%\n",
      "Epoch: 560. Loss: 0.6193. Acc.: 76.38%\n",
      "Epoch: 565. Loss: 0.5415. Acc.: 77.69%\n",
      "Epoch: 570. Loss: 0.4784. Acc.: 78.74%\n",
      "Epoch: 575. Loss: 0.6252. Acc.: 78.22%\n",
      "Epoch: 580. Loss: 0.6303. Acc.: 78.22%\n",
      "Epoch: 585. Loss: 0.4802. Acc.: 78.74%\n",
      "Epoch 589 best model saved with accuracy: 79.79%\n",
      "Epoch: 590. Loss: 0.5604. Acc.: 79.00%\n",
      "Epoch 593 best model saved with accuracy: 80.58%\n",
      "Epoch: 595. Loss: 0.5627. Acc.: 78.74%\n",
      "Epoch: 600. Loss: 0.4819. Acc.: 80.31%\n",
      "Epoch: 605. Loss: 0.5538. Acc.: 79.53%\n",
      "Epoch: 610. Loss: 0.5160. Acc.: 77.43%\n",
      "Epoch: 615. Loss: 0.5553. Acc.: 78.48%\n",
      "Epoch: 620. Loss: 0.3563. Acc.: 79.00%\n",
      "Epoch: 625. Loss: 0.5698. Acc.: 78.48%\n",
      "Epoch: 630. Loss: 0.5460. Acc.: 79.00%\n",
      "Epoch: 635. Loss: 0.5642. Acc.: 78.74%\n",
      "Epoch: 640. Loss: 0.6589. Acc.: 78.48%\n",
      "Epoch: 645. Loss: 0.6015. Acc.: 77.95%\n",
      "Epoch: 650. Loss: 0.6245. Acc.: 77.69%\n",
      "Epoch 653 best model saved with accuracy: 80.84%\n",
      "Epoch 654 best model saved with accuracy: 81.36%\n",
      "Epoch: 655. Loss: 0.5944. Acc.: 80.05%\n",
      "Epoch: 660. Loss: 0.5064. Acc.: 79.79%\n",
      "Epoch: 665. Loss: 0.4891. Acc.: 78.48%\n",
      "Epoch: 670. Loss: 0.5511. Acc.: 78.22%\n",
      "Epoch: 675. Loss: 0.4046. Acc.: 79.00%\n",
      "Epoch: 680. Loss: 0.5322. Acc.: 79.79%\n",
      "Epoch: 685. Loss: 0.4843. Acc.: 77.69%\n",
      "Epoch: 690. Loss: 0.5832. Acc.: 80.58%\n",
      "Epoch: 695. Loss: 0.5666. Acc.: 78.48%\n",
      "Epoch: 700. Loss: 0.4077. Acc.: 77.95%\n",
      "Epoch: 705. Loss: 0.5638. Acc.: 79.53%\n",
      "Epoch: 710. Loss: 0.5262. Acc.: 78.74%\n",
      "Epoch: 715. Loss: 0.4415. Acc.: 78.22%\n",
      "Epoch: 720. Loss: 0.4846. Acc.: 77.95%\n",
      "Epoch: 725. Loss: 0.6413. Acc.: 79.00%\n",
      "Epoch: 730. Loss: 0.5185. Acc.: 77.69%\n",
      "Epoch: 735. Loss: 0.4974. Acc.: 78.48%\n",
      "Epoch: 740. Loss: 0.5473. Acc.: 79.27%\n",
      "Epoch: 745. Loss: 0.4803. Acc.: 80.31%\n",
      "Epoch: 750. Loss: 0.4362. Acc.: 78.48%\n",
      "Epoch 754 best model saved with accuracy: 81.63%\n",
      "Epoch: 755. Loss: 0.5298. Acc.: 80.58%\n",
      "Epoch: 760. Loss: 0.4233. Acc.: 80.84%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 765. Loss: 0.5320. Acc.: 78.74%\n",
      "Epoch: 770. Loss: 0.5284. Acc.: 79.00%\n",
      "Epoch: 775. Loss: 0.4602. Acc.: 78.74%\n",
      "Epoch: 780. Loss: 0.6031. Acc.: 77.43%\n",
      "Epoch: 785. Loss: 0.4820. Acc.: 78.22%\n",
      "Epoch: 790. Loss: 0.5692. Acc.: 79.27%\n",
      "Epoch: 795. Loss: 0.4460. Acc.: 80.58%\n",
      "Epoch: 800. Loss: 0.4375. Acc.: 77.95%\n",
      "Epoch: 805. Loss: 0.5016. Acc.: 79.00%\n",
      "Epoch: 810. Loss: 0.5946. Acc.: 77.95%\n",
      "Epoch: 815. Loss: 0.4850. Acc.: 80.31%\n",
      "Epoch: 820. Loss: 0.4481. Acc.: 78.74%\n",
      "Epoch: 825. Loss: 0.4893. Acc.: 79.27%\n",
      "Epoch: 830. Loss: 0.3550. Acc.: 78.74%\n",
      "Epoch: 835. Loss: 0.4429. Acc.: 81.10%\n",
      "Epoch: 840. Loss: 0.4308. Acc.: 79.53%\n",
      "Epoch: 845. Loss: 0.4799. Acc.: 80.05%\n",
      "Epoch: 850. Loss: 0.4074. Acc.: 80.58%\n",
      "Epoch: 855. Loss: 0.5737. Acc.: 79.79%\n",
      "Epoch: 860. Loss: 0.5978. Acc.: 79.79%\n",
      "Epoch: 865. Loss: 0.4985. Acc.: 79.27%\n",
      "Epoch: 870. Loss: 0.3784. Acc.: 79.27%\n",
      "Epoch: 875. Loss: 0.3345. Acc.: 78.48%\n",
      "Epoch: 880. Loss: 0.4849. Acc.: 79.27%\n",
      "Epoch: 885. Loss: 0.3883. Acc.: 78.74%\n",
      "Epoch: 890. Loss: 0.5482. Acc.: 79.53%\n",
      "Epoch: 895. Loss: 0.3325. Acc.: 79.79%\n",
      "Epoch: 900. Loss: 0.6024. Acc.: 78.22%\n",
      "Epoch: 905. Loss: 0.4302. Acc.: 79.00%\n",
      "Epoch: 910. Loss: 0.3927. Acc.: 80.05%\n",
      "Epoch 912 best model saved with accuracy: 81.89%\n",
      "Epoch: 915. Loss: 0.5119. Acc.: 80.05%\n",
      "Epoch: 920. Loss: 0.4954. Acc.: 78.74%\n",
      "Epoch: 925. Loss: 0.5897. Acc.: 78.74%\n",
      "Epoch: 930. Loss: 0.5013. Acc.: 78.22%\n",
      "Epoch: 935. Loss: 0.5021. Acc.: 79.00%\n",
      "Epoch: 940. Loss: 0.4541. Acc.: 80.05%\n",
      "Epoch: 945. Loss: 0.4773. Acc.: 79.27%\n",
      "Epoch: 950. Loss: 0.5635. Acc.: 80.31%\n",
      "Epoch: 955. Loss: 0.3724. Acc.: 80.05%\n",
      "Epoch: 960. Loss: 0.3985. Acc.: 79.27%\n",
      "Epoch: 965. Loss: 0.5795. Acc.: 79.27%\n",
      "Epoch: 970. Loss: 0.5017. Acc.: 80.58%\n",
      "Epoch: 975. Loss: 0.5061. Acc.: 78.74%\n",
      "Epoch: 980. Loss: 0.4920. Acc.: 80.58%\n",
      "Epoch: 985. Loss: 0.5327. Acc.: 80.84%\n",
      "Epoch: 990. Loss: 0.6447. Acc.: 80.05%\n",
      "Epoch: 995. Loss: 0.5605. Acc.: 79.53%\n",
      "Epoch: 1000. Loss: 0.4283. Acc.: 79.79%\n",
      "Epoch: 1005. Loss: 0.5842. Acc.: 81.89%\n",
      "Epoch: 1010. Loss: 0.4040. Acc.: 80.58%\n",
      "Epoch: 1015. Loss: 0.4258. Acc.: 79.53%\n",
      "Epoch: 1020. Loss: 0.4646. Acc.: 80.31%\n",
      "Epoch: 1025. Loss: 0.4372. Acc.: 78.48%\n",
      "Epoch: 1030. Loss: 0.5484. Acc.: 79.00%\n",
      "Epoch: 1035. Loss: 0.5182. Acc.: 78.22%\n",
      "Epoch: 1040. Loss: 0.5030. Acc.: 80.58%\n",
      "Epoch: 1045. Loss: 0.3329. Acc.: 79.27%\n",
      "Epoch: 1050. Loss: 0.4304. Acc.: 79.00%\n",
      "Epoch: 1055. Loss: 0.4675. Acc.: 81.36%\n",
      "Epoch: 1060. Loss: 0.4228. Acc.: 80.84%\n",
      "Epoch: 1065. Loss: 0.4327. Acc.: 80.05%\n",
      "Epoch: 1070. Loss: 0.4443. Acc.: 81.36%\n",
      "Epoch 1071 best model saved with accuracy: 82.68%\n",
      "Epoch: 1075. Loss: 0.5043. Acc.: 80.84%\n",
      "Epoch: 1080. Loss: 0.4840. Acc.: 78.48%\n",
      "Epoch: 1085. Loss: 0.5516. Acc.: 79.79%\n",
      "Epoch: 1090. Loss: 0.5471. Acc.: 80.05%\n",
      "Epoch: 1095. Loss: 0.4282. Acc.: 79.79%\n",
      "Epoch: 1100. Loss: 0.5234. Acc.: 80.58%\n",
      "Epoch: 1105. Loss: 0.5223. Acc.: 79.27%\n",
      "Epoch: 1110. Loss: 0.3350. Acc.: 79.27%\n",
      "Epoch: 1115. Loss: 0.4727. Acc.: 79.53%\n",
      "Epoch: 1120. Loss: 0.4839. Acc.: 81.63%\n",
      "Epoch: 1125. Loss: 0.4723. Acc.: 79.27%\n",
      "Epoch: 1130. Loss: 0.4468. Acc.: 79.53%\n",
      "Epoch: 1135. Loss: 0.4389. Acc.: 79.79%\n",
      "Epoch: 1140. Loss: 0.3943. Acc.: 79.27%\n",
      "Epoch: 1145. Loss: 0.3864. Acc.: 81.63%\n",
      "Epoch: 1150. Loss: 0.4802. Acc.: 81.63%\n",
      "Epoch: 1155. Loss: 0.5394. Acc.: 80.31%\n",
      "Epoch: 1160. Loss: 0.4103. Acc.: 79.53%\n",
      "Epoch: 1165. Loss: 0.4444. Acc.: 78.48%\n",
      "Epoch: 1170. Loss: 0.4297. Acc.: 79.53%\n",
      "Epoch: 1175. Loss: 0.4962. Acc.: 80.31%\n",
      "Epoch: 1180. Loss: 0.3482. Acc.: 82.15%\n",
      "Epoch: 1185. Loss: 0.5793. Acc.: 80.84%\n",
      "Epoch: 1190. Loss: 0.3782. Acc.: 80.84%\n",
      "Epoch: 1195. Loss: 0.5432. Acc.: 80.31%\n",
      "Epoch: 1200. Loss: 0.4270. Acc.: 80.31%\n",
      "Epoch: 1205. Loss: 0.3432. Acc.: 81.63%\n",
      "Epoch: 1210. Loss: 0.2533. Acc.: 80.58%\n",
      "Epoch: 1215. Loss: 0.5594. Acc.: 80.84%\n",
      "Epoch: 1220. Loss: 0.4635. Acc.: 80.84%\n",
      "Epoch: 1225. Loss: 0.3708. Acc.: 81.89%\n",
      "Epoch: 1230. Loss: 0.4240. Acc.: 81.89%\n",
      "Epoch: 1235. Loss: 0.4524. Acc.: 79.53%\n",
      "Epoch: 1240. Loss: 0.4009. Acc.: 79.27%\n",
      "Epoch: 1245. Loss: 0.4649. Acc.: 80.58%\n",
      "Epoch: 1250. Loss: 0.4118. Acc.: 80.05%\n",
      "Epoch: 1255. Loss: 0.2484. Acc.: 81.89%\n",
      "Epoch: 1260. Loss: 0.3266. Acc.: 80.05%\n",
      "Epoch: 1265. Loss: 0.3863. Acc.: 80.05%\n",
      "Epoch: 1270. Loss: 0.3658. Acc.: 80.31%\n",
      "Epoch: 1275. Loss: 0.4461. Acc.: 82.41%\n",
      "Epoch: 1280. Loss: 0.4267. Acc.: 80.58%\n",
      "Epoch: 1285. Loss: 0.4223. Acc.: 80.31%\n",
      "Epoch: 1290. Loss: 0.5332. Acc.: 79.79%\n",
      "Epoch: 1295. Loss: 0.3547. Acc.: 80.31%\n",
      "Epoch: 1300. Loss: 0.5624. Acc.: 79.00%\n",
      "Epoch: 1305. Loss: 0.4143. Acc.: 82.15%\n",
      "Epoch: 1310. Loss: 0.4596. Acc.: 79.27%\n",
      "Epoch: 1315. Loss: 0.3362. Acc.: 79.53%\n",
      "Epoch: 1320. Loss: 0.5585. Acc.: 79.53%\n",
      "Epoch: 1325. Loss: 0.4464. Acc.: 80.05%\n",
      "Epoch: 1330. Loss: 0.4217. Acc.: 79.00%\n",
      "Epoch: 1335. Loss: 0.4212. Acc.: 80.58%\n",
      "Epoch: 1340. Loss: 0.5618. Acc.: 81.10%\n",
      "Epoch: 1345. Loss: 0.4374. Acc.: 81.36%\n",
      "Epoch: 1350. Loss: 0.4572. Acc.: 81.63%\n",
      "Epoch: 1355. Loss: 0.4057. Acc.: 80.05%\n",
      "Epoch: 1360. Loss: 0.3767. Acc.: 79.79%\n",
      "Epoch: 1365. Loss: 0.4627. Acc.: 79.79%\n",
      "Epoch: 1370. Loss: 0.5495. Acc.: 80.58%\n",
      "Epoch: 1375. Loss: 0.3799. Acc.: 81.10%\n",
      "Epoch: 1380. Loss: 0.3531. Acc.: 80.58%\n",
      "Epoch: 1385. Loss: 0.4203. Acc.: 82.15%\n",
      "Epoch: 1390. Loss: 0.5374. Acc.: 81.10%\n",
      "Epoch: 1395. Loss: 0.3550. Acc.: 82.41%\n",
      "Epoch: 1400. Loss: 0.4088. Acc.: 81.10%\n",
      "Epoch: 1405. Loss: 0.5211. Acc.: 80.05%\n",
      "Epoch: 1410. Loss: 0.3332. Acc.: 79.53%\n",
      "Epoch: 1415. Loss: 0.3203. Acc.: 80.58%\n",
      "Epoch: 1420. Loss: 0.6118. Acc.: 80.05%\n",
      "Epoch: 1425. Loss: 0.3886. Acc.: 80.84%\n",
      "Epoch: 1430. Loss: 0.3882. Acc.: 79.79%\n",
      "Epoch: 1435. Loss: 0.4749. Acc.: 80.31%\n",
      "Epoch: 1440. Loss: 0.3811. Acc.: 80.31%\n",
      "Epoch: 1445. Loss: 0.4422. Acc.: 80.58%\n",
      "Epoch: 1450. Loss: 0.3799. Acc.: 81.89%\n",
      "Epoch: 1455. Loss: 0.4075. Acc.: 79.53%\n",
      "Epoch: 1460. Loss: 0.3511. Acc.: 80.58%\n",
      "Epoch: 1465. Loss: 0.4282. Acc.: 81.10%\n",
      "Epoch: 1470. Loss: 0.3712. Acc.: 81.63%\n",
      "Epoch: 1475. Loss: 0.3310. Acc.: 81.63%\n",
      "Epoch: 1480. Loss: 0.2933. Acc.: 79.53%\n",
      "Epoch: 1485. Loss: 0.4555. Acc.: 80.31%\n",
      "Epoch: 1490. Loss: 0.4122. Acc.: 79.79%\n",
      "Epoch: 1495. Loss: 0.5062. Acc.: 80.05%\n",
      "Epoch: 1500. Loss: 0.4242. Acc.: 80.58%\n",
      "Epoch: 1505. Loss: 0.3990. Acc.: 80.31%\n",
      "Epoch: 1510. Loss: 0.4225. Acc.: 80.05%\n",
      "Epoch: 1515. Loss: 0.4022. Acc.: 80.05%\n",
      "Epoch: 1520. Loss: 0.4026. Acc.: 80.31%\n",
      "Epoch: 1525. Loss: 0.3157. Acc.: 80.58%\n",
      "Epoch: 1530. Loss: 0.3317. Acc.: 82.41%\n",
      "Epoch: 1535. Loss: 0.3170. Acc.: 80.58%\n",
      "Epoch: 1540. Loss: 0.4376. Acc.: 80.05%\n",
      "Epoch: 1545. Loss: 0.5618. Acc.: 81.63%\n",
      "Epoch: 1550. Loss: 0.4558. Acc.: 79.53%\n",
      "Epoch: 1555. Loss: 0.3359. Acc.: 81.63%\n",
      "Epoch: 1560. Loss: 0.4080. Acc.: 80.31%\n",
      "Epoch: 1565. Loss: 0.3898. Acc.: 81.10%\n",
      "Epoch: 1570. Loss: 0.3869. Acc.: 79.53%\n",
      "Early stopping on epoch 1571\n"
     ]
    }
   ],
   "source": [
    "raw_feat = raw_arr.shape[1]\n",
    "fft_feat = fft_arr.shape[1]\n",
    "\n",
    "trn_dl, val_dl, tst_dl = create_loaders(datasets, bs=256)\n",
    "\n",
    "lr = 0.001\n",
    "n_epochs = 3000\n",
    "iterations_per_epoch = len(trn_dl)\n",
    "num_classes = 9\n",
    "best_acc = 0\n",
    "patience, trials = 500, 0\n",
    "\n",
    "model = Classifier(raw_feat, fft_feat, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# sched = CyclicLR(opt, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/100))\n",
    "\n",
    "print('Start model training')\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    model.train()\n",
    "    for i, batch in enumerate(trn_dl):\n",
    "        x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n",
    "        # sched.step()\n",
    "        opt.zero_grad()\n",
    "        out = model(x_raw, x_fft)\n",
    "        loss = criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for batch in val_dl:\n",
    "        x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n",
    "        out = model(x_raw, x_fft)\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "    \n",
    "    acc = correct / total\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch: {epoch:3d}. Loss: {loss.item():.4f}. Acc.: {acc:2.2%}')\n",
    "\n",
    "    if acc > best_acc:\n",
    "        trials = 0\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), 'best.pth')\n",
    "        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7626, 6, 65)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fft_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
