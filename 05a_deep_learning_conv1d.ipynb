{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "def noop(*args, **kwargs): pass\n",
    "warnings.warn = noop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import ChainMap\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basedir import SAMPLE\n",
    "from utils import from_feather, to_feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n",
    "\n",
    "    def __init__(self, keep_batch_dim=True):\n",
    "        super().__init__()\n",
    "        self.keep_batch_dim = keep_batch_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.keep_batch_dim:\n",
    "            return x.view(x.size(0), -1)\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_in, conv=(32, 64, 128, 256, 512), fc=(512, 512, 256), n_out=9):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        fi = n_in\n",
    "        for size in conv:\n",
    "            layers += [\n",
    "                nn.Conv1d(fi, size, 3),\n",
    "                nn.BatchNorm1d(size),\n",
    "                nn.LeakyReLU(0.05)]\n",
    "            fi = size\n",
    "        layers.append(nn.AdaptiveAvgPool1d(1))\n",
    "        layers.append(Flatten())\n",
    "        for size in fc:\n",
    "            layers += [\n",
    "                nn.Linear(fi, size),\n",
    "                nn.BatchNorm1d(size),\n",
    "                nn.LeakyReLU(0.05),\n",
    "                nn.Dropout(0.15)]\n",
    "            fi = size\n",
    "        layers.append(nn.Linear(fi, n_out))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    y_hat = y_pred.reshape(9, n).argmax(axis=0)\n",
    "    value = (y_true == y_hat).mean()\n",
    "    return 'accuracy', value, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + np.cos(np.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_COLS = ['series_id', 'measurement_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grouped_array(data, group_col='series_id', drop_cols=ID_COLS):\n",
    "    X_grouped = np.row_stack([\n",
    "        group.drop(columns=drop_cols).values[None]\n",
    "        for _, group in data.groupby(group_col)])\n",
    "    return X_grouped.transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(X, y, test_size=0.2, dropcols=ID_COLS):\n",
    "    enc = LabelEncoder()\n",
    "    y_enc = enc.fit_transform(y)\n",
    "    X_grouped = create_grouped_array(X)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_grouped, y_enc, test_size=0.1)\n",
    "    X_train, X_valid = [torch.tensor(arr, dtype=torch.float32) for arr in (X_train, X_valid)]\n",
    "    y_train, y_valid = [torch.tensor(arr, dtype=torch.long) for arr in (y_train, y_valid)]\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    valid_ds = TensorDataset(X_valid, y_valid)\n",
    "    return train_ds, valid_ds, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataset(X, dropcols=ID_COLS):\n",
    "    X_grouped = np.row_stack([\n",
    "        group.drop(columns=dropcols).values[None]\n",
    "        for _, group in X.groupby('series_id')])\n",
    "    X_grouped = torch.tensor(X_grouped.transpose(0, 2, 1)).float()\n",
    "    y_fake = torch.tensor([0] * len(X_grouped)).long()\n",
    "    return TensorDataset(X_grouped, y_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(train_ds, valid_ds, bs=512, jobs=0):\n",
    "    train_dl = DataLoader(train_ds, bs, shuffle=True, num_workers=jobs)\n",
    "    valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=jobs)\n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    return (output.argmax(dim=1) == target).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn, y_trn, x_tst = from_feather('x_trn', 'y_trn', 'x_tst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy improvement on epoch: 1\n",
      "[001/1000] train: 2.2650 - val: 1.7709 - val. acc: 30.97%\n",
      "accuracy improvement on epoch: 2\n",
      "[002/1000] train: 1.6682 - val: 1.6052 - val. acc: 37.80%\n",
      "accuracy improvement on epoch: 3\n",
      "[003/1000] train: 1.6563 - val: 1.5750 - val. acc: 40.16%\n",
      "accuracy improvement on epoch: 4\n",
      "[004/1000] train: 1.5265 - val: 1.5203 - val. acc: 44.09%\n",
      "[005/1000] train: 1.6318 - val: 1.4842 - val. acc: 41.73%\n",
      "accuracy improvement on epoch: 6\n",
      "[006/1000] train: 1.4218 - val: 1.4392 - val. acc: 46.72%\n",
      "accuracy improvement on epoch: 7\n",
      "[007/1000] train: 1.5578 - val: 1.4493 - val. acc: 49.08%\n",
      "[008/1000] train: 1.3835 - val: 1.4069 - val. acc: 47.51%\n",
      "[009/1000] train: 1.4783 - val: 1.4185 - val. acc: 48.03%\n",
      "accuracy improvement on epoch: 10\n",
      "[010/1000] train: 1.3502 - val: 1.3730 - val. acc: 50.39%\n",
      "[011/1000] train: 1.4132 - val: 1.3935 - val. acc: 49.61%\n",
      "accuracy improvement on epoch: 12\n",
      "[012/1000] train: 1.3053 - val: 1.3655 - val. acc: 51.97%\n",
      "[013/1000] train: 1.3726 - val: 1.3307 - val. acc: 51.97%\n",
      "accuracy improvement on epoch: 14\n",
      "[014/1000] train: 1.2868 - val: 1.3242 - val. acc: 54.59%\n",
      "[015/1000] train: 1.3460 - val: 1.3317 - val. acc: 50.13%\n",
      "[016/1000] train: 1.2603 - val: 1.3053 - val. acc: 52.76%\n",
      "[017/1000] train: 1.3681 - val: 1.3459 - val. acc: 50.92%\n",
      "[018/1000] train: 1.2468 - val: 1.3367 - val. acc: 52.49%\n",
      "[019/1000] train: 1.3140 - val: 1.3369 - val. acc: 51.97%\n",
      "[020/1000] train: 1.2127 - val: 1.3061 - val. acc: 50.92%\n",
      "[021/1000] train: 1.2974 - val: 1.2963 - val. acc: 54.33%\n",
      "[022/1000] train: 1.2049 - val: 1.3066 - val. acc: 54.33%\n",
      "[023/1000] train: 1.2837 - val: 1.3056 - val. acc: 54.59%\n",
      "accuracy improvement on epoch: 24\n",
      "[024/1000] train: 1.1841 - val: 1.2683 - val. acc: 55.38%\n",
      "[025/1000] train: 1.2539 - val: 1.2890 - val. acc: 52.49%\n",
      "[026/1000] train: 1.1355 - val: 1.2656 - val. acc: 52.49%\n",
      "accuracy improvement on epoch: 27\n",
      "[027/1000] train: 1.2464 - val: 1.2521 - val. acc: 55.64%\n",
      "accuracy improvement on epoch: 28\n",
      "[028/1000] train: 1.1114 - val: 1.2363 - val. acc: 56.96%\n",
      "[029/1000] train: 1.2967 - val: 1.2555 - val. acc: 54.59%\n",
      "[030/1000] train: 1.1253 - val: 1.2252 - val. acc: 56.69%\n",
      "[031/1000] train: 1.1977 - val: 1.2710 - val. acc: 55.12%\n",
      "[032/1000] train: 1.0678 - val: 1.2037 - val. acc: 55.12%\n",
      "[033/1000] train: 1.1820 - val: 1.2348 - val. acc: 56.69%\n",
      "accuracy improvement on epoch: 34\n",
      "[034/1000] train: 1.0320 - val: 1.1590 - val. acc: 59.58%\n",
      "[035/1000] train: 1.1347 - val: 1.2174 - val. acc: 55.64%\n",
      "accuracy improvement on epoch: 36\n",
      "[036/1000] train: 1.0033 - val: 1.1414 - val. acc: 60.89%\n",
      "[037/1000] train: 1.1684 - val: 1.2475 - val. acc: 58.01%\n",
      "[038/1000] train: 1.0066 - val: 1.1425 - val. acc: 58.53%\n",
      "[039/1000] train: 1.0918 - val: 1.1664 - val. acc: 57.48%\n",
      "accuracy improvement on epoch: 40\n",
      "[040/1000] train: 0.9489 - val: 1.0845 - val. acc: 62.99%\n",
      "[041/1000] train: 1.1027 - val: 1.1529 - val. acc: 60.10%\n",
      "[042/1000] train: 0.9308 - val: 1.1029 - val. acc: 62.20%\n",
      "[043/1000] train: 1.0807 - val: 1.0991 - val. acc: 61.15%\n",
      "[044/1000] train: 0.9189 - val: 1.0808 - val. acc: 62.73%\n",
      "[045/1000] train: 1.0852 - val: 1.1171 - val. acc: 58.27%\n",
      "[046/1000] train: 0.9014 - val: 1.0592 - val. acc: 62.99%\n",
      "[047/1000] train: 1.0474 - val: 1.1010 - val. acc: 59.06%\n",
      "[048/1000] train: 0.8980 - val: 1.0549 - val. acc: 62.99%\n",
      "[049/1000] train: 1.0700 - val: 1.0850 - val. acc: 61.15%\n",
      "accuracy improvement on epoch: 50\n",
      "[050/1000] train: 0.8668 - val: 1.0506 - val. acc: 63.25%\n",
      "[051/1000] train: 1.0085 - val: 1.0578 - val. acc: 61.42%\n",
      "[052/1000] train: 0.8422 - val: 1.0475 - val. acc: 61.94%\n",
      "[053/1000] train: 0.9917 - val: 1.0362 - val. acc: 63.25%\n",
      "accuracy improvement on epoch: 54\n",
      "[054/1000] train: 0.8121 - val: 0.9955 - val. acc: 64.57%\n",
      "[055/1000] train: 0.9610 - val: 1.0042 - val. acc: 63.52%\n",
      "accuracy improvement on epoch: 56\n",
      "[056/1000] train: 0.7921 - val: 0.9982 - val. acc: 65.09%\n",
      "[057/1000] train: 0.9077 - val: 1.0247 - val. acc: 64.30%\n",
      "[058/1000] train: 0.7754 - val: 1.0222 - val. acc: 64.04%\n",
      "[059/1000] train: 0.9824 - val: 1.0407 - val. acc: 62.20%\n",
      "accuracy improvement on epoch: 60\n",
      "[060/1000] train: 0.7793 - val: 0.9795 - val. acc: 65.88%\n",
      "[061/1000] train: 0.8792 - val: 1.0096 - val. acc: 65.09%\n",
      "accuracy improvement on epoch: 62\n",
      "[062/1000] train: 0.7301 - val: 0.9747 - val. acc: 67.72%\n",
      "[063/1000] train: 0.9828 - val: 0.9844 - val. acc: 65.88%\n",
      "accuracy improvement on epoch: 64\n",
      "[064/1000] train: 0.7452 - val: 0.9458 - val. acc: 69.29%\n",
      "[065/1000] train: 0.8754 - val: 0.9636 - val. acc: 66.67%\n",
      "[066/1000] train: 0.7308 - val: 0.9331 - val. acc: 65.09%\n",
      "[067/1000] train: 0.8602 - val: 0.9947 - val. acc: 64.83%\n",
      "[068/1000] train: 0.6964 - val: 0.9431 - val. acc: 67.19%\n",
      "[069/1000] train: 0.8223 - val: 0.9254 - val. acc: 67.72%\n",
      "[070/1000] train: 0.6813 - val: 0.9222 - val. acc: 69.29%\n",
      "[071/1000] train: 0.9213 - val: 0.9480 - val. acc: 66.67%\n",
      "[072/1000] train: 0.6839 - val: 0.9144 - val. acc: 69.03%\n",
      "[073/1000] train: 0.8057 - val: 0.9446 - val. acc: 66.67%\n",
      "[074/1000] train: 0.6712 - val: 0.9220 - val. acc: 67.45%\n",
      "[075/1000] train: 0.7595 - val: 0.9872 - val. acc: 66.40%\n",
      "accuracy improvement on epoch: 76\n",
      "[076/1000] train: 0.6448 - val: 0.9001 - val. acc: 70.08%\n",
      "[077/1000] train: 0.9211 - val: 0.9665 - val. acc: 65.88%\n",
      "[078/1000] train: 0.6848 - val: 0.9123 - val. acc: 69.55%\n",
      "[079/1000] train: 0.7796 - val: 0.9083 - val. acc: 67.98%\n",
      "[080/1000] train: 0.6366 - val: 0.8923 - val. acc: 69.29%\n",
      "[081/1000] train: 0.7315 - val: 0.9638 - val. acc: 65.62%\n",
      "accuracy improvement on epoch: 82\n",
      "[082/1000] train: 0.6040 - val: 0.8878 - val. acc: 71.39%\n",
      "[083/1000] train: 0.7647 - val: 0.9426 - val. acc: 68.24%\n",
      "[084/1000] train: 0.6006 - val: 0.8842 - val. acc: 69.29%\n",
      "[085/1000] train: 0.7625 - val: 0.9158 - val. acc: 69.55%\n",
      "[086/1000] train: 0.5878 - val: 0.8843 - val. acc: 69.29%\n",
      "[087/1000] train: 0.7557 - val: 0.9121 - val. acc: 68.24%\n",
      "[088/1000] train: 0.5781 - val: 0.8937 - val. acc: 69.82%\n",
      "[089/1000] train: 0.6748 - val: 0.9721 - val. acc: 68.24%\n",
      "[090/1000] train: 0.5523 - val: 0.8859 - val. acc: 68.77%\n",
      "[091/1000] train: 0.7401 - val: 0.9251 - val. acc: 69.55%\n",
      "[092/1000] train: 0.5355 - val: 0.8744 - val. acc: 70.87%\n",
      "[093/1000] train: 0.7177 - val: 0.8836 - val. acc: 71.39%\n",
      "accuracy improvement on epoch: 94\n",
      "[094/1000] train: 0.5473 - val: 0.8405 - val. acc: 72.97%\n",
      "[095/1000] train: 0.7137 - val: 0.8610 - val. acc: 70.34%\n",
      "[096/1000] train: 0.5239 - val: 0.8278 - val. acc: 72.44%\n",
      "[097/1000] train: 0.7921 - val: 0.8811 - val. acc: 70.60%\n",
      "[098/1000] train: 0.5666 - val: 0.8342 - val. acc: 70.60%\n",
      "[099/1000] train: 0.6053 - val: 0.8406 - val. acc: 71.39%\n",
      "[100/1000] train: 0.5142 - val: 0.8678 - val. acc: 71.65%\n",
      "[101/1000] train: 0.6519 - val: 0.9197 - val. acc: 69.29%\n",
      "[102/1000] train: 0.4944 - val: 0.8609 - val. acc: 70.08%\n",
      "[103/1000] train: 0.7117 - val: 0.9119 - val. acc: 67.98%\n",
      "[104/1000] train: 0.5181 - val: 0.8390 - val. acc: 70.87%\n",
      "[105/1000] train: 0.6188 - val: 0.8542 - val. acc: 70.08%\n",
      "[106/1000] train: 0.4778 - val: 0.8241 - val. acc: 71.92%\n",
      "[107/1000] train: 0.7077 - val: 0.7846 - val. acc: 72.97%\n",
      "accuracy improvement on epoch: 108\n",
      "[108/1000] train: 0.4809 - val: 0.8068 - val. acc: 73.75%\n",
      "[109/1000] train: 0.6138 - val: 0.8423 - val. acc: 70.87%\n",
      "[110/1000] train: 0.4782 - val: 0.8357 - val. acc: 71.13%\n",
      "[111/1000] train: 0.5480 - val: 0.8820 - val. acc: 70.34%\n",
      "[112/1000] train: 0.4325 - val: 0.8013 - val. acc: 72.97%\n",
      "[113/1000] train: 0.6136 - val: 0.9267 - val. acc: 69.82%\n",
      "accuracy improvement on epoch: 114\n",
      "[114/1000] train: 0.4631 - val: 0.8066 - val. acc: 74.54%\n",
      "[115/1000] train: 0.5779 - val: 0.8262 - val. acc: 72.18%\n",
      "[116/1000] train: 0.4251 - val: 0.7863 - val. acc: 74.28%\n",
      "[117/1000] train: 0.5389 - val: 0.8090 - val. acc: 72.18%\n",
      "[118/1000] train: 0.3958 - val: 0.7875 - val. acc: 72.44%\n",
      "[119/1000] train: 0.6004 - val: 0.8144 - val. acc: 71.92%\n",
      "[120/1000] train: 0.4095 - val: 0.7819 - val. acc: 74.28%\n",
      "[121/1000] train: 0.5740 - val: 0.8799 - val. acc: 70.60%\n",
      "[122/1000] train: 0.4255 - val: 0.7640 - val. acc: 71.92%\n",
      "[123/1000] train: 0.4601 - val: 0.8425 - val. acc: 74.02%\n",
      "[124/1000] train: 0.3954 - val: 0.7719 - val. acc: 73.23%\n",
      "[125/1000] train: 0.5407 - val: 0.8350 - val. acc: 72.18%\n",
      "accuracy improvement on epoch: 126\n",
      "[126/1000] train: 0.4099 - val: 0.7830 - val. acc: 74.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127/1000] train: 0.5253 - val: 0.7588 - val. acc: 74.54%\n",
      "[128/1000] train: 0.3597 - val: 0.7798 - val. acc: 73.23%\n",
      "[129/1000] train: 0.5423 - val: 0.8167 - val. acc: 74.80%\n",
      "accuracy improvement on epoch: 130\n",
      "[130/1000] train: 0.3655 - val: 0.7770 - val. acc: 75.33%\n",
      "[131/1000] train: 0.4612 - val: 0.7704 - val. acc: 75.33%\n",
      "accuracy improvement on epoch: 132\n",
      "[132/1000] train: 0.3397 - val: 0.7435 - val. acc: 77.95%\n",
      "[133/1000] train: 0.5380 - val: 0.7947 - val. acc: 72.97%\n",
      "[134/1000] train: 0.3530 - val: 0.7329 - val. acc: 74.54%\n",
      "accuracy improvement on epoch: 135\n",
      "[135/1000] train: 0.4418 - val: 0.7249 - val. acc: 79.00%\n",
      "[136/1000] train: 0.3354 - val: 0.7326 - val. acc: 76.64%\n",
      "[137/1000] train: 0.4679 - val: 0.8224 - val. acc: 73.23%\n",
      "[138/1000] train: 0.3338 - val: 0.7316 - val. acc: 78.22%\n",
      "[139/1000] train: 0.5340 - val: 0.7683 - val. acc: 73.23%\n",
      "[140/1000] train: 0.3441 - val: 0.7492 - val. acc: 76.12%\n",
      "[141/1000] train: 0.4991 - val: 0.7952 - val. acc: 74.80%\n",
      "[142/1000] train: 0.3263 - val: 0.7580 - val. acc: 75.85%\n",
      "[143/1000] train: 0.5210 - val: 0.8774 - val. acc: 71.39%\n",
      "[144/1000] train: 0.3170 - val: 0.7929 - val. acc: 74.02%\n",
      "[145/1000] train: 0.4636 - val: 0.7714 - val. acc: 76.38%\n",
      "[146/1000] train: 0.2979 - val: 0.7701 - val. acc: 76.38%\n",
      "[147/1000] train: 0.4829 - val: 0.8673 - val. acc: 72.97%\n",
      "[148/1000] train: 0.3244 - val: 0.7662 - val. acc: 75.59%\n",
      "[149/1000] train: 0.3879 - val: 0.7245 - val. acc: 75.85%\n",
      "[150/1000] train: 0.2881 - val: 0.7280 - val. acc: 77.69%\n",
      "[151/1000] train: 0.4621 - val: 0.7308 - val. acc: 76.90%\n",
      "[152/1000] train: 0.2973 - val: 0.7093 - val. acc: 77.69%\n",
      "[153/1000] train: 0.5439 - val: 0.6874 - val. acc: 77.17%\n",
      "[154/1000] train: 0.3071 - val: 0.7210 - val. acc: 77.17%\n",
      "[155/1000] train: 0.3492 - val: 0.7709 - val. acc: 75.59%\n",
      "[156/1000] train: 0.2763 - val: 0.7268 - val. acc: 76.64%\n",
      "[157/1000] train: 0.3910 - val: 0.7518 - val. acc: 77.43%\n",
      "[158/1000] train: 0.2642 - val: 0.7031 - val. acc: 78.48%\n",
      "[159/1000] train: 0.5045 - val: 0.7726 - val. acc: 75.59%\n",
      "[160/1000] train: 0.3056 - val: 0.6887 - val. acc: 77.95%\n",
      "[161/1000] train: 0.3858 - val: 0.7256 - val. acc: 76.12%\n",
      "[162/1000] train: 0.2480 - val: 0.6991 - val. acc: 76.90%\n",
      "[163/1000] train: 0.3803 - val: 0.7652 - val. acc: 77.17%\n",
      "accuracy improvement on epoch: 164\n",
      "[164/1000] train: 0.2519 - val: 0.6857 - val. acc: 80.31%\n",
      "[165/1000] train: 0.4354 - val: 0.7039 - val. acc: 79.27%\n",
      "[166/1000] train: 0.2465 - val: 0.6738 - val. acc: 80.05%\n",
      "[167/1000] train: 0.3605 - val: 0.6691 - val. acc: 77.69%\n",
      "[168/1000] train: 0.2374 - val: 0.7228 - val. acc: 78.22%\n",
      "[169/1000] train: 0.3686 - val: 0.7471 - val. acc: 77.17%\n",
      "[170/1000] train: 0.2352 - val: 0.7242 - val. acc: 77.69%\n",
      "[171/1000] train: 0.2997 - val: 0.8425 - val. acc: 75.33%\n",
      "[172/1000] train: 0.2290 - val: 0.7333 - val. acc: 79.00%\n",
      "[173/1000] train: 0.4783 - val: 0.6565 - val. acc: 79.53%\n",
      "[174/1000] train: 0.2409 - val: 0.7090 - val. acc: 80.31%\n",
      "[175/1000] train: 0.3561 - val: 0.7423 - val. acc: 77.95%\n",
      "[176/1000] train: 0.2134 - val: 0.7444 - val. acc: 79.27%\n",
      "[177/1000] train: 0.3638 - val: 0.7692 - val. acc: 77.69%\n",
      "[178/1000] train: 0.2247 - val: 0.7275 - val. acc: 79.53%\n",
      "[179/1000] train: 0.4477 - val: 0.7683 - val. acc: 73.49%\n",
      "[180/1000] train: 0.2470 - val: 0.7278 - val. acc: 77.69%\n",
      "[181/1000] train: 0.3207 - val: 0.7297 - val. acc: 76.90%\n",
      "accuracy improvement on epoch: 182\n",
      "[182/1000] train: 0.2309 - val: 0.6554 - val. acc: 81.10%\n",
      "[183/1000] train: 0.2969 - val: 0.8017 - val. acc: 76.38%\n",
      "[184/1000] train: 0.2082 - val: 0.7138 - val. acc: 79.00%\n",
      "[185/1000] train: 0.3452 - val: 0.7402 - val. acc: 77.95%\n",
      "[186/1000] train: 0.2286 - val: 0.7119 - val. acc: 79.79%\n",
      "[187/1000] train: 0.3812 - val: 0.8213 - val. acc: 78.48%\n",
      "[188/1000] train: 0.1975 - val: 0.7204 - val. acc: 80.84%\n",
      "[189/1000] train: 0.2942 - val: 0.7791 - val. acc: 77.69%\n",
      "[190/1000] train: 0.1841 - val: 0.7449 - val. acc: 80.05%\n",
      "[191/1000] train: 0.3975 - val: 0.6740 - val. acc: 78.74%\n",
      "[192/1000] train: 0.1910 - val: 0.7614 - val. acc: 78.74%\n",
      "[193/1000] train: 0.3020 - val: 0.8664 - val. acc: 76.64%\n",
      "[194/1000] train: 0.1950 - val: 0.7207 - val. acc: 79.79%\n",
      "[195/1000] train: 0.3295 - val: 0.7552 - val. acc: 79.27%\n",
      "accuracy improvement on epoch: 196\n",
      "[196/1000] train: 0.1917 - val: 0.6784 - val. acc: 81.63%\n",
      "[197/1000] train: 0.4024 - val: 0.7740 - val. acc: 75.85%\n",
      "[198/1000] train: 0.1880 - val: 0.7014 - val. acc: 79.27%\n",
      "[199/1000] train: 0.3243 - val: 0.7176 - val. acc: 78.74%\n",
      "accuracy improvement on epoch: 200\n",
      "[200/1000] train: 0.1885 - val: 0.6731 - val. acc: 82.94%\n",
      "[201/1000] train: 0.2663 - val: 0.8671 - val. acc: 75.33%\n",
      "[202/1000] train: 0.1755 - val: 0.7460 - val. acc: 80.05%\n",
      "[203/1000] train: 0.3355 - val: 0.7961 - val. acc: 76.64%\n",
      "[204/1000] train: 0.1956 - val: 0.7299 - val. acc: 79.53%\n",
      "[205/1000] train: 0.2968 - val: 0.7630 - val. acc: 77.95%\n",
      "[206/1000] train: 0.1926 - val: 0.7003 - val. acc: 80.31%\n",
      "[207/1000] train: 0.3694 - val: 0.7304 - val. acc: 80.31%\n",
      "[208/1000] train: 0.1748 - val: 0.7329 - val. acc: 79.79%\n",
      "[209/1000] train: 0.2683 - val: 0.8420 - val. acc: 75.85%\n",
      "[210/1000] train: 0.1932 - val: 0.6882 - val. acc: 80.31%\n",
      "[211/1000] train: 0.2174 - val: 0.6870 - val. acc: 81.63%\n",
      "[212/1000] train: 0.1531 - val: 0.6969 - val. acc: 80.58%\n",
      "[213/1000] train: 0.3288 - val: 0.8932 - val. acc: 76.38%\n",
      "[214/1000] train: 0.1773 - val: 0.7273 - val. acc: 81.36%\n",
      "[215/1000] train: 0.3015 - val: 0.7809 - val. acc: 78.48%\n",
      "[216/1000] train: 0.1699 - val: 0.7132 - val. acc: 82.41%\n",
      "[217/1000] train: 0.5122 - val: 0.6791 - val. acc: 79.79%\n",
      "[218/1000] train: 0.1842 - val: 0.6687 - val. acc: 80.84%\n",
      "[219/1000] train: 0.2113 - val: 0.8371 - val. acc: 76.12%\n",
      "[220/1000] train: 0.1665 - val: 0.7164 - val. acc: 79.79%\n",
      "[221/1000] train: 0.2164 - val: 0.7659 - val. acc: 77.17%\n",
      "[222/1000] train: 0.1420 - val: 0.6867 - val. acc: 80.31%\n",
      "[223/1000] train: 0.2567 - val: 0.7879 - val. acc: 79.00%\n",
      "[224/1000] train: 0.1550 - val: 0.6664 - val. acc: 80.31%\n",
      "[225/1000] train: 0.3497 - val: 0.7715 - val. acc: 78.22%\n",
      "[226/1000] train: 0.1576 - val: 0.7231 - val. acc: 79.27%\n",
      "[227/1000] train: 0.4270 - val: 0.6395 - val. acc: 81.10%\n",
      "[228/1000] train: 0.1653 - val: 0.6830 - val. acc: 79.79%\n",
      "[229/1000] train: 0.2000 - val: 0.7716 - val. acc: 79.27%\n",
      "[230/1000] train: 0.1332 - val: 0.6753 - val. acc: 82.15%\n",
      "[231/1000] train: 0.2862 - val: 0.7603 - val. acc: 78.22%\n",
      "[232/1000] train: 0.1460 - val: 0.7372 - val. acc: 81.10%\n",
      "[233/1000] train: 0.1971 - val: 0.7657 - val. acc: 79.79%\n",
      "[234/1000] train: 0.1619 - val: 0.7788 - val. acc: 80.31%\n",
      "[235/1000] train: 0.2752 - val: 0.8015 - val. acc: 76.64%\n",
      "[236/1000] train: 0.1430 - val: 0.6845 - val. acc: 80.84%\n",
      "[237/1000] train: 0.3157 - val: 0.7065 - val. acc: 80.05%\n",
      "[238/1000] train: 0.1400 - val: 0.6749 - val. acc: 80.31%\n",
      "[239/1000] train: 0.2422 - val: 0.7439 - val. acc: 79.79%\n",
      "[240/1000] train: 0.1347 - val: 0.7143 - val. acc: 80.58%\n",
      "[241/1000] train: 0.2915 - val: 0.6881 - val. acc: 78.74%\n",
      "[242/1000] train: 0.1536 - val: 0.6493 - val. acc: 81.63%\n",
      "accuracy improvement on epoch: 243\n",
      "[243/1000] train: 0.2133 - val: 0.6752 - val. acc: 83.46%\n",
      "[244/1000] train: 0.1270 - val: 0.7445 - val. acc: 82.15%\n",
      "[245/1000] train: 0.2022 - val: 0.7727 - val. acc: 79.00%\n",
      "[246/1000] train: 0.1392 - val: 0.7357 - val. acc: 80.31%\n",
      "[247/1000] train: 0.2557 - val: 0.9153 - val. acc: 76.64%\n",
      "[248/1000] train: 0.1704 - val: 0.7174 - val. acc: 79.79%\n",
      "[249/1000] train: 0.1579 - val: 0.7605 - val. acc: 81.36%\n",
      "[250/1000] train: 0.1118 - val: 0.6936 - val. acc: 82.68%\n",
      "[251/1000] train: 0.2052 - val: 0.7084 - val. acc: 79.79%\n",
      "[252/1000] train: 0.1252 - val: 0.7169 - val. acc: 81.10%\n",
      "[253/1000] train: 0.3236 - val: 0.7184 - val. acc: 80.05%\n",
      "[254/1000] train: 0.1376 - val: 0.7127 - val. acc: 81.36%\n",
      "[255/1000] train: 0.2188 - val: 0.7050 - val. acc: 79.79%\n",
      "[256/1000] train: 0.1348 - val: 0.6751 - val. acc: 79.79%\n",
      "[257/1000] train: 0.1509 - val: 0.7336 - val. acc: 79.79%\n",
      "[258/1000] train: 0.1384 - val: 0.6884 - val. acc: 81.10%\n",
      "[259/1000] train: 0.2398 - val: 0.8096 - val. acc: 78.22%\n",
      "[260/1000] train: 0.1210 - val: 0.6853 - val. acc: 81.36%\n",
      "[261/1000] train: 0.3906 - val: 0.6596 - val. acc: 80.31%\n",
      "[262/1000] train: 0.1515 - val: 0.6168 - val. acc: 82.41%\n",
      "[263/1000] train: 0.1948 - val: 0.7220 - val. acc: 81.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[264/1000] train: 0.1145 - val: 0.7365 - val. acc: 81.89%\n",
      "[265/1000] train: 0.2371 - val: 0.7690 - val. acc: 80.05%\n",
      "[266/1000] train: 0.1103 - val: 0.6617 - val. acc: 81.63%\n",
      "[267/1000] train: 0.1649 - val: 0.8066 - val. acc: 79.53%\n",
      "[268/1000] train: 0.1124 - val: 0.6981 - val. acc: 81.89%\n",
      "[269/1000] train: 0.1441 - val: 0.7121 - val. acc: 80.84%\n",
      "[270/1000] train: 0.0919 - val: 0.7699 - val. acc: 81.10%\n",
      "[271/1000] train: 0.2045 - val: 1.0037 - val. acc: 76.64%\n",
      "[272/1000] train: 0.1234 - val: 0.8145 - val. acc: 79.00%\n",
      "[273/1000] train: 0.2794 - val: 0.8269 - val. acc: 78.48%\n",
      "[274/1000] train: 0.1411 - val: 0.6632 - val. acc: 81.36%\n",
      "[275/1000] train: 0.2335 - val: 0.7925 - val. acc: 80.31%\n",
      "[276/1000] train: 0.1252 - val: 0.7789 - val. acc: 80.58%\n",
      "[277/1000] train: 0.1445 - val: 0.8515 - val. acc: 77.43%\n",
      "[278/1000] train: 0.1151 - val: 0.8016 - val. acc: 80.05%\n",
      "[279/1000] train: 0.2502 - val: 0.8360 - val. acc: 79.79%\n",
      "[280/1000] train: 0.1225 - val: 0.7740 - val. acc: 80.84%\n",
      "[281/1000] train: 0.1280 - val: 0.8611 - val. acc: 78.74%\n",
      "[282/1000] train: 0.0932 - val: 0.8234 - val. acc: 81.89%\n",
      "[283/1000] train: 0.1528 - val: 0.8855 - val. acc: 79.27%\n",
      "[284/1000] train: 0.1002 - val: 0.7980 - val. acc: 80.84%\n",
      "[285/1000] train: 0.1681 - val: 0.8173 - val. acc: 79.27%\n",
      "[286/1000] train: 0.0883 - val: 0.7429 - val. acc: 82.41%\n",
      "[287/1000] train: 0.3512 - val: 0.8258 - val. acc: 75.59%\n",
      "[288/1000] train: 0.1270 - val: 0.7153 - val. acc: 81.63%\n",
      "[289/1000] train: 0.2526 - val: 0.6830 - val. acc: 82.94%\n",
      "[290/1000] train: 0.1156 - val: 0.6693 - val. acc: 81.63%\n",
      "[291/1000] train: 0.1708 - val: 0.7438 - val. acc: 81.10%\n",
      "[292/1000] train: 0.0898 - val: 0.6791 - val. acc: 80.84%\n",
      "[293/1000] train: 0.1642 - val: 0.7869 - val. acc: 82.15%\n",
      "[294/1000] train: 0.1065 - val: 0.7082 - val. acc: 83.46%\n",
      "[295/1000] train: 0.2035 - val: 0.7979 - val. acc: 80.05%\n",
      "[296/1000] train: 0.0962 - val: 0.7014 - val. acc: 81.63%\n",
      "[297/1000] train: 0.2878 - val: 0.8409 - val. acc: 77.95%\n",
      "[298/1000] train: 0.1275 - val: 0.7403 - val. acc: 79.27%\n",
      "[299/1000] train: 0.1492 - val: 0.6927 - val. acc: 82.15%\n",
      "[300/1000] train: 0.0953 - val: 0.6958 - val. acc: 82.15%\n",
      "[301/1000] train: 0.1061 - val: 0.7888 - val. acc: 81.10%\n",
      "[302/1000] train: 0.0884 - val: 0.7577 - val. acc: 81.89%\n",
      "[303/1000] train: 0.2135 - val: 0.9055 - val. acc: 78.48%\n",
      "[304/1000] train: 0.0989 - val: 0.7681 - val. acc: 80.31%\n",
      "[305/1000] train: 0.2879 - val: 0.7035 - val. acc: 82.68%\n",
      "[306/1000] train: 0.0997 - val: 0.6656 - val. acc: 82.41%\n",
      "[307/1000] train: 0.1592 - val: 0.7374 - val. acc: 80.05%\n",
      "[308/1000] train: 0.0837 - val: 0.6790 - val. acc: 81.89%\n",
      "[309/1000] train: 0.2130 - val: 0.7635 - val. acc: 82.15%\n",
      "[310/1000] train: 0.0903 - val: 0.6627 - val. acc: 82.41%\n",
      "[311/1000] train: 0.1259 - val: 0.7787 - val. acc: 79.53%\n",
      "[312/1000] train: 0.0938 - val: 0.7356 - val. acc: 82.41%\n",
      "[313/1000] train: 0.2251 - val: 0.6708 - val. acc: 83.20%\n",
      "[314/1000] train: 0.1045 - val: 0.6642 - val. acc: 83.20%\n",
      "[315/1000] train: 0.2056 - val: 0.7737 - val. acc: 80.31%\n",
      "[316/1000] train: 0.0895 - val: 0.7523 - val. acc: 80.84%\n",
      "[317/1000] train: 0.1296 - val: 0.7722 - val. acc: 79.53%\n",
      "[318/1000] train: 0.0864 - val: 0.6988 - val. acc: 82.15%\n",
      "[319/1000] train: 0.2938 - val: 0.8080 - val. acc: 77.69%\n",
      "accuracy improvement on epoch: 320\n",
      "[320/1000] train: 0.1160 - val: 0.7006 - val. acc: 83.99%\n",
      "[321/1000] train: 0.2915 - val: 0.7711 - val. acc: 77.95%\n",
      "[322/1000] train: 0.1109 - val: 0.7188 - val. acc: 81.10%\n",
      "[323/1000] train: 0.1449 - val: 0.7941 - val. acc: 80.31%\n",
      "[324/1000] train: 0.0883 - val: 0.7399 - val. acc: 81.10%\n",
      "[325/1000] train: 0.1755 - val: 0.6960 - val. acc: 81.36%\n",
      "[326/1000] train: 0.0950 - val: 0.7520 - val. acc: 80.84%\n",
      "[327/1000] train: 0.1298 - val: 0.7606 - val. acc: 82.94%\n",
      "[328/1000] train: 0.0849 - val: 0.7860 - val. acc: 81.10%\n",
      "[329/1000] train: 0.1245 - val: 0.8374 - val. acc: 80.05%\n",
      "[330/1000] train: 0.0688 - val: 0.7886 - val. acc: 81.36%\n",
      "[331/1000] train: 0.1108 - val: 0.8209 - val. acc: 80.58%\n",
      "[332/1000] train: 0.0802 - val: 0.7522 - val. acc: 83.73%\n",
      "[333/1000] train: 0.2442 - val: 0.8212 - val. acc: 77.17%\n",
      "[334/1000] train: 0.0819 - val: 0.7117 - val. acc: 83.20%\n",
      "[335/1000] train: 0.1887 - val: 0.8277 - val. acc: 80.31%\n",
      "[336/1000] train: 0.0749 - val: 0.7848 - val. acc: 81.10%\n",
      "[337/1000] train: 0.1414 - val: 1.0116 - val. acc: 76.64%\n",
      "[338/1000] train: 0.0874 - val: 0.7888 - val. acc: 83.73%\n",
      "[339/1000] train: 0.2045 - val: 0.7452 - val. acc: 80.84%\n",
      "[340/1000] train: 0.1111 - val: 0.7321 - val. acc: 82.15%\n",
      "[341/1000] train: 0.1867 - val: 0.7758 - val. acc: 81.36%\n",
      "[342/1000] train: 0.0756 - val: 0.6881 - val. acc: 81.10%\n",
      "[343/1000] train: 0.0699 - val: 0.7522 - val. acc: 81.63%\n",
      "[344/1000] train: 0.0531 - val: 0.7618 - val. acc: 81.36%\n",
      "[345/1000] train: 0.2022 - val: 0.8293 - val. acc: 78.22%\n",
      "[346/1000] train: 0.0951 - val: 0.6870 - val. acc: 81.36%\n",
      "[347/1000] train: 0.1572 - val: 0.7515 - val. acc: 81.89%\n",
      "[348/1000] train: 0.0698 - val: 0.7243 - val. acc: 82.41%\n",
      "[349/1000] train: 0.1673 - val: 0.8892 - val. acc: 78.22%\n",
      "[350/1000] train: 0.0751 - val: 0.7444 - val. acc: 81.36%\n",
      "[351/1000] train: 0.1376 - val: 0.7941 - val. acc: 80.31%\n",
      "[352/1000] train: 0.0740 - val: 0.7633 - val. acc: 83.20%\n",
      "[353/1000] train: 0.1302 - val: 0.7695 - val. acc: 81.10%\n",
      "[354/1000] train: 0.0753 - val: 0.6979 - val. acc: 82.15%\n",
      "[355/1000] train: 0.4111 - val: 0.8027 - val. acc: 75.59%\n",
      "[356/1000] train: 0.1471 - val: 0.7272 - val. acc: 79.79%\n",
      "[357/1000] train: 0.1111 - val: 0.7611 - val. acc: 79.79%\n",
      "[358/1000] train: 0.0609 - val: 0.7699 - val. acc: 81.10%\n",
      "[359/1000] train: 0.1261 - val: 0.8312 - val. acc: 80.84%\n",
      "[360/1000] train: 0.0718 - val: 0.7117 - val. acc: 81.63%\n",
      "[361/1000] train: 0.0952 - val: 0.7195 - val. acc: 82.94%\n",
      "[362/1000] train: 0.0636 - val: 0.7161 - val. acc: 82.68%\n",
      "[363/1000] train: 0.1560 - val: 0.6471 - val. acc: 81.10%\n",
      "[364/1000] train: 0.0660 - val: 0.6650 - val. acc: 81.36%\n",
      "[365/1000] train: 0.2432 - val: 0.7650 - val. acc: 81.36%\n",
      "[366/1000] train: 0.0899 - val: 0.7565 - val. acc: 81.89%\n",
      "[367/1000] train: 0.0870 - val: 0.9651 - val. acc: 78.48%\n",
      "[368/1000] train: 0.0841 - val: 0.7781 - val. acc: 82.15%\n",
      "[369/1000] train: 0.2723 - val: 0.6835 - val. acc: 81.89%\n",
      "[370/1000] train: 0.0908 - val: 0.6321 - val. acc: 82.68%\n",
      "[371/1000] train: 0.0697 - val: 0.7338 - val. acc: 81.89%\n",
      "[372/1000] train: 0.0769 - val: 0.6918 - val. acc: 81.36%\n",
      "[373/1000] train: 0.1294 - val: 0.9308 - val. acc: 78.22%\n",
      "[374/1000] train: 0.0800 - val: 0.7585 - val. acc: 81.89%\n",
      "[375/1000] train: 0.1757 - val: 0.7482 - val. acc: 81.89%\n",
      "[376/1000] train: 0.0731 - val: 0.7714 - val. acc: 83.46%\n",
      "[377/1000] train: 0.1248 - val: 0.6888 - val. acc: 83.73%\n",
      "[378/1000] train: 0.0664 - val: 0.6939 - val. acc: 82.68%\n",
      "[379/1000] train: 0.0712 - val: 0.9504 - val. acc: 79.53%\n",
      "[380/1000] train: 0.0467 - val: 0.7644 - val. acc: 81.63%\n",
      "[381/1000] train: 0.2083 - val: 0.8030 - val. acc: 79.53%\n",
      "[382/1000] train: 0.0975 - val: 0.6708 - val. acc: 83.99%\n",
      "[383/1000] train: 0.1175 - val: 0.7189 - val. acc: 82.94%\n",
      "[384/1000] train: 0.0655 - val: 0.6746 - val. acc: 83.20%\n",
      "[385/1000] train: 0.1808 - val: 0.7355 - val. acc: 82.41%\n",
      "[386/1000] train: 0.0845 - val: 0.6880 - val. acc: 83.46%\n",
      "[387/1000] train: 0.1966 - val: 0.9018 - val. acc: 79.53%\n",
      "[388/1000] train: 0.0856 - val: 0.7138 - val. acc: 81.36%\n",
      "[389/1000] train: 0.0809 - val: 0.8500 - val. acc: 81.63%\n",
      "[390/1000] train: 0.0597 - val: 0.6948 - val. acc: 83.20%\n",
      "[391/1000] train: 0.0749 - val: 0.7627 - val. acc: 82.15%\n",
      "[392/1000] train: 0.0566 - val: 0.7744 - val. acc: 80.84%\n",
      "[393/1000] train: 0.1843 - val: 0.8742 - val. acc: 79.27%\n",
      "[394/1000] train: 0.0736 - val: 0.7225 - val. acc: 82.41%\n",
      "[395/1000] train: 0.1563 - val: 0.8743 - val. acc: 80.05%\n",
      "[396/1000] train: 0.0695 - val: 0.7470 - val. acc: 79.53%\n",
      "[397/1000] train: 0.0640 - val: 0.8238 - val. acc: 79.00%\n",
      "[398/1000] train: 0.0438 - val: 0.7770 - val. acc: 82.68%\n",
      "[399/1000] train: 0.1369 - val: 0.8247 - val. acc: 80.58%\n",
      "[400/1000] train: 0.0714 - val: 0.8146 - val. acc: 79.00%\n",
      "[401/1000] train: 0.0862 - val: 0.9603 - val. acc: 79.00%\n",
      "[402/1000] train: 0.0591 - val: 0.7758 - val. acc: 80.84%\n",
      "[403/1000] train: 0.1222 - val: 0.9239 - val. acc: 81.10%\n",
      "[404/1000] train: 0.0656 - val: 0.8801 - val. acc: 80.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[405/1000] train: 0.2525 - val: 0.8302 - val. acc: 79.79%\n",
      "[406/1000] train: 0.0839 - val: 0.7504 - val. acc: 81.10%\n",
      "[407/1000] train: 0.1295 - val: 0.7997 - val. acc: 82.94%\n",
      "[408/1000] train: 0.0655 - val: 0.7312 - val. acc: 83.99%\n",
      "[409/1000] train: 0.0870 - val: 0.9212 - val. acc: 78.74%\n",
      "[410/1000] train: 0.0618 - val: 0.7544 - val. acc: 80.84%\n",
      "[411/1000] train: 0.0980 - val: 1.0551 - val. acc: 79.00%\n",
      "[412/1000] train: 0.0680 - val: 0.8884 - val. acc: 80.05%\n",
      "[413/1000] train: 0.0680 - val: 0.8675 - val. acc: 81.10%\n",
      "[414/1000] train: 0.0423 - val: 0.9059 - val. acc: 80.05%\n",
      "[415/1000] train: 0.2648 - val: 0.6686 - val. acc: 82.68%\n",
      "[416/1000] train: 0.0666 - val: 0.6693 - val. acc: 82.15%\n",
      "[417/1000] train: 0.1062 - val: 0.7406 - val. acc: 82.15%\n",
      "[418/1000] train: 0.0509 - val: 0.7383 - val. acc: 81.89%\n",
      "[419/1000] train: 0.1382 - val: 0.8361 - val. acc: 80.84%\n",
      "[420/1000] train: 0.0677 - val: 0.7736 - val. acc: 82.68%\n",
      "early stopping after epoch 420\n",
      "Loading the best weights with the validation accuracy: 83.99%\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "wd = 1e-5\n",
    "bs = 400\n",
    "n_epochs = 1000\n",
    "patience = 100\n",
    "no_improvements = 0\n",
    "jobs = 12\n",
    "best_loss = np.inf\n",
    "best_acc = 0\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "trn_ds, val_ds, enc = create_datasets(x_trn, y_trn['surface'])\n",
    "trn_dl, val_dl = create_loaders(trn_ds, val_ds, bs, jobs=jobs)\n",
    "dataset_sizes = {'train': len(trn_ds), 'val': len(val_ds)}\n",
    "\n",
    "model = Classifier(10)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=wd)\n",
    "iterations_per_epoch = len(trn_dl)\n",
    "scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/100))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "    \n",
    "    for phase, loader in (('train', trn_dl), ('val', val_dl)):\n",
    "        training = phase == 'train'\n",
    "        running_loss = 0.0\n",
    "        num_correct = 0\n",
    "        num_examples = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in loader:\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = model(x_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    lr_history.extend(scheduler.get_lr())\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            num_correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
    "            num_examples += len(x_batch)\n",
    "            \n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        \n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            val_acc = num_correct/num_examples\n",
    "            # if epoch_loss < best_loss:\n",
    "            if val_acc > best_acc:\n",
    "                # print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                print('accuracy improvement on epoch: %d' % (epoch + 1))\n",
    "                best_acc = val_acc\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_weights, 'best_weights.pth')\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "            stats['val_acc'] = val_acc\n",
    "                \n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] '\n",
    "          'train: {train:.4f} - '\n",
    "          'val: {val:.4f} - '\n",
    "          'val. acc: {val_acc:2.2%}'.format(**stats))\n",
    "    \n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break\n",
    "\n",
    "if best_weights is not None:\n",
    "    # print(f'Loading the best weights with the training loss: {best_loss:.4f}')\n",
    "    print(f'Loading the best weights with the validation accuracy: {best_acc:2.2%}')\n",
    "    model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(10).to(device)\n",
    "model.load_state_dict(torch.load('best_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers = model.layers[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, device):\n",
    "    return torch.tensor(create_grouped_array(data)).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X, weights_file, device='cpu'):\n",
    "    model = Classifier(10).to(device)\n",
    "    model.load_state_dict(torch.load(weights_file))\n",
    "    model.layers = model.layers[:-1]\n",
    "    model.eval()\n",
    "    input_tensor = prepare_data(X, device)\n",
    "    feat = model(input_tensor)\n",
    "    return feat.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn_tensor = torch.tensor(create_grouped_array(x_trn)).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, filename in ((x_trn, 'trn_feat.npy'), (x_tst, 'tst_feat.npy')):\n",
    "    feat = extract_features(data, 'best_weights.pth')\n",
    "    np.save(filename, feat.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_feat = model(prepare_data(x_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = create_test_dataset(x_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, enc = create_datasets(x_trn, y_trn['surface'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "model.eval()\n",
    "for x_batch, _ in DataLoader(test_ds, batch_size=1000, shuffle=False):\n",
    "    output = model(x_batch.to(device))\n",
    "    test_results += output.argmax(dim=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 51.9k/51.9k [00:00<00:00, 47.3kB/s]\n",
      "Successfully submitted to CareerCon 2019 - Help Navigate Robots "
     ]
    }
   ],
   "source": [
    "submit = pd.read_csv(SAMPLE)\n",
    "submit['surface'] = enc.inverse_transform(test_results)\n",
    "submit.to_csv('submit.csv', index=None)\n",
    "!kaggle c submit career-con-2019 -f 'submit.csv' -m \"Conv1d deeper (ensure eval mode)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
