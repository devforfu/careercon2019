{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "def noop(*args, **kwargs): pass\n",
    "warnings.warn = noop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import ChainMap\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basedir import SAMPLE\n",
    "from utils import from_feather, to_feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n",
    "\n",
    "    def __init__(self, keep_batch_dim=True):\n",
    "        super().__init__()\n",
    "        self.keep_batch_dim = keep_batch_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.keep_batch_dim:\n",
    "            return x.view(x.size(0), -1)\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_in, conv=(32, 64, 128, 256), fc=(512, 512), n_out=9):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        fi = n_in\n",
    "        for size in conv:\n",
    "            layers += [\n",
    "                nn.Conv1d(fi, size, 3),\n",
    "                nn.BatchNorm1d(size),\n",
    "                nn.LeakyReLU(0.05)]\n",
    "            fi = size\n",
    "        layers.append(nn.AdaptiveAvgPool1d(1))\n",
    "        layers.append(Flatten())\n",
    "        for size in fc:\n",
    "            layers += [\n",
    "                nn.Linear(fi, size),\n",
    "                nn.BatchNorm1d(size),\n",
    "                nn.LeakyReLU(0.05),\n",
    "                nn.Dropout(0.15)]\n",
    "            fi = size\n",
    "        layers.append(nn.Linear(fi, n_out))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    y_hat = y_pred.reshape(9, n).argmax(axis=0)\n",
    "    value = (y_true == y_hat).mean()\n",
    "    return 'accuracy', value, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + np.cos(np.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_COLS = ['series_id', 'measurement_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(X, y, test_size=0.2, dropcols=ID_COLS):\n",
    "    enc = LabelEncoder()\n",
    "    y_enc = enc.fit_transform(y)\n",
    "    X_grouped = np.row_stack([\n",
    "        group.drop(columns=dropcols).values[None]\n",
    "        for _, group in X.groupby('series_id')])\n",
    "    X_grouped = X_grouped.transpose(0, 2, 1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_grouped, y_enc, test_size=0.1)\n",
    "    X_train, X_valid = [torch.tensor(arr, dtype=torch.float32) for arr in (X_train, X_valid)]\n",
    "    y_train, y_valid = [torch.tensor(arr, dtype=torch.long) for arr in (y_train, y_valid)]\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    valid_ds = TensorDataset(X_valid, y_valid)\n",
    "    return train_ds, valid_ds, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataset(X, dropcols=ID_COLS):\n",
    "    X_grouped = np.row_stack([\n",
    "        group.drop(columns=dropcols).values[None]\n",
    "        for _, group in X.groupby('series_id')])\n",
    "    X_grouped = torch.tensor(X_grouped.transpose(0, 2, 1)).float()\n",
    "    y_fake = torch.tensor([0] * len(X_grouped)).long()\n",
    "    return TensorDataset(X_grouped, y_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(train_ds, valid_ds, bs=512, jobs=0):\n",
    "    train_dl = DataLoader(train_ds, bs, shuffle=True, num_workers=jobs)\n",
    "    valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=jobs)\n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    return (output.argmax(dim=1) == target).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn, y_trn, x_tst = from_feather('x_trn', 'y_trn', 'x_tst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "wd = 1e-5\n",
    "bs = 400\n",
    "n_epochs = 1000\n",
    "patience = 200\n",
    "no_improvements = 0\n",
    "jobs = 12\n",
    "best_loss = np.inf\n",
    "best_acc = 0\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "trn_ds, val_ds, enc = create_datasets(x_trn, y_trn['surface'])\n",
    "trn_dl, val_dl = create_loaders(trn_ds, val_ds, bs, jobs=jobs)\n",
    "dataset_sizes = {'train': len(trn_ds), 'val': len(val_ds)}\n",
    "\n",
    "model = Classifier(10)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=wd)\n",
    "iterations_per_epoch = len(trn_dl)\n",
    "scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/100))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "    \n",
    "    for phase, loader in (('train', trn_dl), ('val', val_dl)):\n",
    "        training = phase == 'train'\n",
    "        running_loss = 0.0\n",
    "        num_correct = 0\n",
    "        num_examples = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in loader:\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = model(x_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    lr_history.extend(scheduler.get_lr())\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            num_correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
    "            num_examples += len(x_batch)\n",
    "            \n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        \n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            val_acc = num_correct/num_examples\n",
    "            # if epoch_loss < best_loss:\n",
    "            if val_acc > best_acc:\n",
    "                # print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                print('accuracy improvement on epoch: %d' % (epoch + 1))\n",
    "                best_acc = val_acc\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_weights, 'best_weights.pth')\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "            stats['val_acc'] = val_acc\n",
    "                \n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] '\n",
    "          'train: {train:.4f} - '\n",
    "          'val: {val:.4f} - '\n",
    "          'val. acc: {val_acc:2.2%}'.format(**stats))\n",
    "    \n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break\n",
    "\n",
    "if best_weights is not None:\n",
    "    # print(f'Loading the best weights with the training loss: {best_loss:.4f}')\n",
    "    print(f'Loading the best weights with the validation accuracy: {best_acc:2.2%}')\n",
    "    model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(10).to(device)\n",
    "model.load_state_dict(torch.load('best_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = create_test_dataset(x_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, _, enc = create_datasets(x_trn, y_trn['surface'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "for x_batch, _ in DataLoader(test_ds, batch_size=1000, shuffle=False):\n",
    "    output = model(x_batch.to(device))\n",
    "    test_results += output.argmax(dim=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(SAMPLE)\n",
    "submit['surface'] = enc.inverse_transform(test_results)\n",
    "submit.to_csv('submit.csv', index=None)\n",
    "!kaggle c submit career-con-2019 -f 'submit.csv' -m \"Conv1d first try\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
