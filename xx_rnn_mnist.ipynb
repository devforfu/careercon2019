{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = datasets.MNIST(root=Path.home()/'data'/'MNIST',\n",
    "                        train=True,\n",
    "                        transform=T.ToTensor(),\n",
    "                        download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_ds = datasets.MNIST(root=Path.home()/'data'/'MNIST',\n",
    "                        train=False,\n",
    "                        transform=T.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]), torch.Size([60000]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds.data.size(), trn_ds.targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 28, 28]), torch.Size([10000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_ds.data.size(), tst_ds.targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim,\n",
    "                          batch_first=True, nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "        h0.requires_grad_()\n",
    "        h0 = h0.cuda()\n",
    "        out, hn = self.rnn(x, h0.detach())\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interpret MNIST images as sequence of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28    # MNIST image width (input features)\n",
    "hidden_dim = 100  # number of units per hidden layer of RNN\n",
    "layer_dim = 1     # number of hidden layers\n",
    "output_dim = 10   # number of MNIST classes\n",
    "seq_dim = 28      # MNIST image height (timestaps to unroll)\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model = model.cuda()\n",
    "lr = 0.01\n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "bs = 100\n",
    "n_iters = 3000\n",
    "n_epochs = int(n_iters / (len(trn_ds) / bs))\n",
    "trn_loader = torch.utils.data.DataLoader(\n",
    "    dataset=trn_ds, batch_size=bs, shuffle=True)\n",
    "tst_loader = torch.utils.data.DataLoader(\n",
    "    dataset=tst_ds, batch_size=bs, shuffle=False)\n",
    "\n",
    "c = 0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (images, labels) in enumerate(trn_loader):\n",
    "        model.train()\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "        opt.zero_grad()\n",
    "        outputs = model(images.cuda())\n",
    "        loss = criterion(outputs, labels.cuda())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        c += 1\n",
    "        \n",
    "        if c % 500 == 0:\n",
    "            model.eval()\n",
    "            correct, total = 0, 0\n",
    "            for images, labels in tst_loader:\n",
    "                images = images.view(-1, seq_dim, input_dim)\n",
    "                outputs = model(images.cuda())\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels.cuda()).sum()\n",
    "            acc = 100 * correct / total\n",
    "            print(f'Iteration: {c}. Loss: {loss.item():.4f}. Acc.: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the same approach to the competition's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import from_feather\n",
    "from torch_helpers import create_loaders\n",
    "from torch_helpers import create_datasets, create_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.batch_size = None\n",
    "        self.hidden = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0, c0 = self.init_hidden(x)\n",
    "        out, (hn, cn) = self.rnn(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        # self.hidden = hn, cn\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, x):\n",
    "#         if self.batch_size is not None and self.batch_size == x.size(0):\n",
    "#             return self.hidden\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "        return [t.cuda() for t in (h0, c0)]\n",
    "        \n",
    "#         self.batch_size = x.size(0)\n",
    "#         self.hidden = [t.to('cuda') for t in (h0, c0)]\n",
    "#         return self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn, y_trn, x_tst = from_feather('x_trn', 'y_trn', 'x_tst')\n",
    "\n",
    "trn_ds, val_ds, enc = create_datasets(x_trn, y_trn['surface'])\n",
    "\n",
    "bs = 128\n",
    "trn_dl, val_dl = create_loaders(trn_ds, val_ds, bs, jobs=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + np.cos(np.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 best model saved: lstm_best_0.2100.pth\n",
      "Epoch 2 best model saved: lstm_best_0.2651.pth\n",
      "Epoch:   5. Loss: 1.8100. Acc.: 27.82%\n",
      "Epoch 5 best model saved: lstm_best_0.2782.pth\n",
      "Epoch 6 best model saved: lstm_best_0.3202.pth\n",
      "Epoch 9 best model saved: lstm_best_0.3333.pth\n",
      "Epoch:  10. Loss: 1.6520. Acc.: 37.80%\n",
      "Epoch 10 best model saved: lstm_best_0.3780.pth\n",
      "Epoch 12 best model saved: lstm_best_0.4042.pth\n",
      "Epoch 13 best model saved: lstm_best_0.4383.pth\n",
      "Epoch 14 best model saved: lstm_best_0.4409.pth\n",
      "Epoch:  15. Loss: 1.4330. Acc.: 44.88%\n",
      "Epoch 15 best model saved: lstm_best_0.4488.pth\n",
      "Epoch 18 best model saved: lstm_best_0.4514.pth\n",
      "Epoch:  20. Loss: 1.2123. Acc.: 47.51%\n",
      "Epoch 20 best model saved: lstm_best_0.4751.pth\n",
      "Epoch 21 best model saved: lstm_best_0.5118.pth\n",
      "Epoch 22 best model saved: lstm_best_0.5512.pth\n",
      "Epoch:  25. Loss: 1.3888. Acc.: 53.02%\n",
      "Epoch 26 best model saved: lstm_best_0.5801.pth\n",
      "Epoch 28 best model saved: lstm_best_0.5906.pth\n",
      "Epoch:  30. Loss: 1.0516. Acc.: 61.42%\n",
      "Epoch 30 best model saved: lstm_best_0.6142.pth\n",
      "Epoch:  35. Loss: 1.0985. Acc.: 60.37%\n",
      "Epoch 36 best model saved: lstm_best_0.6194.pth\n",
      "Epoch 38 best model saved: lstm_best_0.6247.pth\n",
      "Epoch:  40. Loss: 1.0481. Acc.: 63.52%\n",
      "Epoch 40 best model saved: lstm_best_0.6352.pth\n",
      "Epoch:  45. Loss: 1.0826. Acc.: 59.32%\n",
      "Epoch 46 best model saved: lstm_best_0.6378.pth\n",
      "Epoch 48 best model saved: lstm_best_0.6509.pth\n",
      "Epoch:  50. Loss: 0.9011. Acc.: 64.04%\n",
      "Epoch 52 best model saved: lstm_best_0.6745.pth\n",
      "Epoch 54 best model saved: lstm_best_0.6929.pth\n",
      "Epoch:  55. Loss: 1.1799. Acc.: 62.20%\n",
      "Epoch:  60. Loss: 0.7819. Acc.: 66.14%\n",
      "Epoch:  65. Loss: 1.0143. Acc.: 64.30%\n",
      "Epoch 68 best model saved: lstm_best_0.7060.pth\n",
      "Epoch:  70. Loss: 0.6149. Acc.: 71.39%\n",
      "Epoch 70 best model saved: lstm_best_0.7139.pth\n",
      "Epoch 72 best model saved: lstm_best_0.7323.pth\n",
      "Epoch:  75. Loss: 0.9415. Acc.: 60.63%\n",
      "Epoch:  80. Loss: 0.7226. Acc.: 72.70%\n",
      "Epoch:  85. Loss: 0.5839. Acc.: 68.50%\n",
      "Epoch 86 best model saved: lstm_best_0.7480.pth\n",
      "Epoch 88 best model saved: lstm_best_0.7507.pth\n",
      "Epoch:  90. Loss: 0.6110. Acc.: 74.28%\n",
      "Epoch:  95. Loss: 0.5904. Acc.: 72.44%\n",
      "Epoch 96 best model saved: lstm_best_0.7559.pth\n",
      "Epoch: 100. Loss: 0.7475. Acc.: 71.13%\n",
      "Epoch: 105. Loss: 0.5534. Acc.: 73.75%\n",
      "Epoch 106 best model saved: lstm_best_0.7690.pth\n",
      "Epoch 108 best model saved: lstm_best_0.7717.pth\n",
      "Epoch: 110. Loss: 0.4966. Acc.: 77.17%\n",
      "Epoch 114 best model saved: lstm_best_0.7822.pth\n",
      "Epoch: 115. Loss: 0.5328. Acc.: 76.12%\n",
      "Epoch: 120. Loss: 0.5249. Acc.: 77.17%\n",
      "Epoch: 125. Loss: 0.4271. Acc.: 74.54%\n",
      "Epoch: 130. Loss: 0.2717. Acc.: 77.43%\n",
      "Epoch: 135. Loss: 0.3253. Acc.: 79.00%\n",
      "Epoch 135 best model saved: lstm_best_0.7900.pth\n",
      "Epoch: 140. Loss: 0.2388. Acc.: 78.22%\n",
      "Epoch: 145. Loss: 0.5150. Acc.: 73.75%\n",
      "Epoch: 150. Loss: 0.3162. Acc.: 76.64%\n",
      "Epoch 154 best model saved: lstm_best_0.7953.pth\n",
      "Epoch: 155. Loss: 0.4166. Acc.: 77.17%\n",
      "Epoch: 160. Loss: 0.2333. Acc.: 79.27%\n",
      "Epoch: 165. Loss: 0.3053. Acc.: 75.59%\n",
      "Epoch 166 best model saved: lstm_best_0.8058.pth\n",
      "Epoch: 170. Loss: 0.1565. Acc.: 81.36%\n",
      "Epoch 170 best model saved: lstm_best_0.8136.pth\n",
      "Epoch: 175. Loss: 0.2204. Acc.: 78.22%\n",
      "Epoch: 180. Loss: 0.0911. Acc.: 80.58%\n",
      "Epoch: 185. Loss: 0.3137. Acc.: 76.38%\n",
      "Epoch: 190. Loss: 0.1445. Acc.: 78.22%\n",
      "Epoch: 195. Loss: 0.1642. Acc.: 80.31%\n",
      "Epoch: 200. Loss: 0.0851. Acc.: 79.79%\n",
      "Epoch 204 best model saved: lstm_best_0.8215.pth\n",
      "Epoch: 205. Loss: 0.1940. Acc.: 80.84%\n",
      "Epoch: 210. Loss: 0.0409. Acc.: 79.53%\n",
      "Epoch: 215. Loss: 0.1058. Acc.: 78.48%\n",
      "Epoch: 220. Loss: 0.1602. Acc.: 77.95%\n",
      "Epoch: 225. Loss: 0.0813. Acc.: 76.12%\n",
      "Epoch: 230. Loss: 0.0313. Acc.: 81.89%\n",
      "Epoch: 235. Loss: 0.0881. Acc.: 79.53%\n",
      "Epoch: 240. Loss: 0.0671. Acc.: 80.58%\n",
      "Epoch: 245. Loss: 0.1716. Acc.: 76.12%\n",
      "Epoch: 250. Loss: 0.0457. Acc.: 78.74%\n",
      "Epoch: 255. Loss: 0.3852. Acc.: 76.12%\n",
      "Epoch: 260. Loss: 0.0434. Acc.: 78.74%\n",
      "Epoch: 265. Loss: 0.1396. Acc.: 76.38%\n",
      "Epoch: 270. Loss: 0.0256. Acc.: 79.53%\n",
      "Epoch: 275. Loss: 0.0287. Acc.: 80.05%\n",
      "Epoch: 280. Loss: 0.0226. Acc.: 81.63%\n",
      "Epoch: 285. Loss: 0.0805. Acc.: 78.22%\n",
      "Epoch: 290. Loss: 0.0071. Acc.: 81.89%\n",
      "Epoch: 295. Loss: 0.0596. Acc.: 79.00%\n",
      "Epoch: 300. Loss: 0.0157. Acc.: 80.31%\n",
      "Early stopping on epoch 304\n"
     ]
    }
   ],
   "source": [
    "input_dim = 10    \n",
    "hidden_dim = 256\n",
    "layer_dim = 3\n",
    "output_dim = 9\n",
    "seq_dim = 128\n",
    "lr = 0.0005\n",
    "n_iters = 3000\n",
    "\n",
    "n_epochs = 1000\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model = model.cuda()\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "iterations_per_epoch = len(trn_dl)\n",
    "sched = CyclicLR(opt, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/100))\n",
    "patience, trials = 100, 0\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    for i, (x_batch, y_batch) in enumerate(trn_dl):\n",
    "        model.train()\n",
    "        x_batch = x_batch.cuda()\n",
    "        y_batch = y_batch.cuda()\n",
    "        sched.step()\n",
    "        opt.zero_grad()\n",
    "        out = model(x_batch)\n",
    "        loss = criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x_val, y_val in val_dl:\n",
    "        x_val, y_val = [t.cuda() for t in (x_val, y_val)]\n",
    "        out = model(x_val)\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        total += y_val.size(0)\n",
    "        correct += (preds == y_val).sum().item()\n",
    "    \n",
    "    acc = correct / total\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch: {epoch:3d}. Loss: {loss.item():.4f}. Acc.: {acc:2.2%}')\n",
    "\n",
    "    if acc > best_acc:\n",
    "        trials = 0\n",
    "        best_acc = acc\n",
    "        filename = f'lstm_best_{acc:0.4f}.pth'\n",
    "        torch.save(model.state_dict(), filename)\n",
    "        print(f'Epoch {epoch} best model saved: {filename}')\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('lstm_best_0.8215.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_ds = create_test_dataset(x_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (rnn): LSTM(10, 256, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "for x, _ in DataLoader(tst_ds, batch_size=256, shuffle=False):\n",
    "    x = x.permute(0, 2, 1)\n",
    "    out = model(x.cuda())\n",
    "    y_hat = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "    test_results.extend(y_hat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from basedir import SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 53.1k/53.1k [00:00<00:00, 48.3kB/s]\n",
      "Successfully submitted to CareerCon 2019 - Help Navigate Robots "
     ]
    }
   ],
   "source": [
    "submit = pd.read_csv(SAMPLE)\n",
    "submit['surface'] = enc.inverse_transform(test_results)\n",
    "submit.to_csv('submit.csv', index=None)\n",
    "!kaggle c submit career-con-2019 -f 'submit.csv' -m \"LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
