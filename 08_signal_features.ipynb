{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "def noop(*args, **kwargs): pass\n",
    "warnings.warn = noop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import ChainMap\n",
    "from itertools import product\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split, ParameterSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tsfresh.feature_extraction.feature_calculators import *\n",
    "from tsfresh.feature_selection.relevance import calculate_relevance_table\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basedir import SAMPLE\n",
    "from info import ID_COLS\n",
    "from lightgbm_helpers import accuracy\n",
    "from utils import to_feather, from_feather, split, kfolds, replace_not_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat(f, **params):\n",
    "    def wrapper(x):\n",
    "        return f(x, **params)\n",
    "    wrapper.__name__ = f.__name__\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class StatsFeatures:\n",
    "    def __init__(self, funcs):\n",
    "        self.funcs = funcs\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        features = {}\n",
    "        for col in data.columns:\n",
    "            for func in self.funcs:\n",
    "                result = func(data[col].values)\n",
    "                if type(result) == zip:\n",
    "                    result = dict(result)\n",
    "                    for key, value in result.items():\n",
    "                        features[f'{col}__{key}'] = value\n",
    "                elif hasattr(result, '__len__'):\n",
    "                    for key, value in result:\n",
    "                        features[f'{col}__{func.__name__}__{key}'] = value\n",
    "                else:\n",
    "                    features[f'{col}__{func.__name__}'] = result\n",
    "        features = {\n",
    "            k: int(v) if v in (True, False) else v \n",
    "            for k, v in features.items()}\n",
    "        return features\n",
    "    \n",
    "class SliceFeatures:\n",
    "    def __init__(self, mode='first', n=5):\n",
    "        if mode not in {'first', 'middle', 'last'}:\n",
    "            raise ValueError('unexpected mode')\n",
    "        self.mode = mode\n",
    "        self.n = n\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        if self.mode == 'first':\n",
    "            start, end = 0, self.n\n",
    "        elif self.mode == 'last':\n",
    "            start, end = -self.n, len(data)\n",
    "        elif self.mode == 'middle':\n",
    "            mid = len(data) // 2\n",
    "            div, mod = divmod(self.n, 2)\n",
    "            start, end = mid-div, mid+div+mod\n",
    "        cols = data.columns\n",
    "        vec = data.iloc[start:end].values.T.ravel()\n",
    "        new_cols = [f'{col}_{self.mode}{i}' for i in range(self.n) for col in cols]\n",
    "        return dict(zip(new_cols, vec))\n",
    "\n",
    "    \n",
    "def add_euler_angles(X):\n",
    "    X = X.copy()\n",
    "    x, y, z, w = [X[f'orientation_{s}'] for s in list('XYZW')]\n",
    "    nx, ny, nz = quaternion_to_euler(x, y, z, w)\n",
    "    X['euler_X'] = nx\n",
    "    X['euler_Y'] = ny\n",
    "    X['euler_Z'] = nz\n",
    "    return X\n",
    "\n",
    "\n",
    "def quaternion_to_euler(x, y, z, w):\n",
    "    t0 = 2.0*(w*x + y*z)\n",
    "    t1 = 1.0 - 2.0*(x*x + y*y)\n",
    "    X = np.arctan2(t0, t1)\n",
    "    \n",
    "    t2 = np.clip(2.0*(w*y - z*x), -1, 1)\n",
    "    Y = np.arcsin(t2)\n",
    "    \n",
    "    t3 = 2.0*(w*z + x*y)\n",
    "    t4 = 1.0 - 2.0*(y*y + z*z)\n",
    "    Z = np.arctan2(t3, t4)\n",
    "    \n",
    "    return X, Y, Z\n",
    "    \n",
    "    \n",
    "def generate_features(data, features, ignore=None):\n",
    "    with Parallel(n_jobs=cpu_count()) as parallel:\n",
    "        extracted = parallel(delayed(generate_features_for_group)(\n",
    "            group=group.drop(columns=ignore or []),\n",
    "            features=features\n",
    "        ) for _, group in tqdm(data.groupby('series_id')))\n",
    "    return pd.DataFrame(extracted)\n",
    "\n",
    "\n",
    "def generate_features_for_group(group, features):\n",
    "    return dict(ChainMap(*[feat(group) for feat in features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Calculation Functions Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features = [\n",
    "    StatsFeatures(funcs=(\n",
    "        stat(fft_aggregated, param=[\n",
    "            {'aggtype': s} \n",
    "            for s in ('centroid', 'variance', 'skew', 'kurtosis')\n",
    "        ]),\n",
    "        stat(fft_coefficient, param=[\n",
    "            {'coeff': k, 'attr': a}\n",
    "            for k, a in product(range(100), ('real', 'imag', 'abs', 'angle'))\n",
    "        ]),\n",
    "        # more features\n",
    "        stat(cwt_coefficients, param=[\n",
    "            {'widths': width, 'coeff': coef, 'w': w}\n",
    "            for width in [(2, 5, 10, 20)]\n",
    "            for coeff in range(15)\n",
    "            for w in (2, 5, 10, 20)\n",
    "        ]),\n",
    "        stat(spkt_welch_density, param=[{'coeff': k} for k in (2, 5, 8)])\n",
    "    )), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfresh_default = [\n",
    "    *[stat(time_reversal_asymmetry_statistic, lag=lag) for lag in range(1, 4)],\n",
    "    *[stat(c3, lag=lag) for lag in range(1, 4)],\n",
    "    stat(cid_ce, normalize=True), stat(cid_ce, normalize=False),\n",
    "    stat(symmetry_looking, param=[{'r': r*0.05} for r in range(1, 20)]),\n",
    "    *[stat(quantile, q=q) for q in (.1, .2, .3, .4, .6, .7, .8, .9)],\n",
    "    *[stat(autocorrelation, lag=lag) for lag in range(10)],\n",
    "    stat(agg_autocorrelation, param=[\n",
    "        {'f_agg': s, 'maxlag': 40} for s in ('mean', 'median', 'var')\n",
    "    ]),\n",
    "    stat(partial_autocorrelation, param=[\n",
    "        {'lag': lag} for lag in range(10)\n",
    "    ]),\n",
    "    *[stat(number_cwt_peaks, n=n) for n in (1, 5)],\n",
    "    *[stat(number_peaks, n=n) for n in (1, 3, 5, 10, 50)],\n",
    "    *[stat(binned_entropy, max_bins=b) for b in [10]],\n",
    "    stat(index_mass_quantile, param=[{'q': q} for q in (.1, .2, .3, .4, .6, .7, .8, .9)]),\n",
    "    stat(cwt_coefficients, param=[\n",
    "        {'widths': width, 'coeff': coeff, 'w': w}\n",
    "        for width in [(2, 5, 10, 20)]\n",
    "        for coeff in range(15)\n",
    "        for w in (2, 5, 10, 20)\n",
    "    ]),\n",
    "    stat(spkt_welch_density, param=[{'coeff': k} for k in (2, 5, 8)]),\n",
    "    stat(ar_coefficient, param=[\n",
    "        {'coeff': coeff, 'k': k} for coeff in range(5) for k in [10]\n",
    "    ]),\n",
    "    *[stat(change_quantiles, ql=ql, qh=qh, isabs=b, f_agg=f)\n",
    "        for ql in (0., .2, .4, .6, .8)\n",
    "        for qh in (.2, .4, .6, .8, 1.)\n",
    "        for b in (False, True)\n",
    "        for f in ('mean', 'var')\n",
    "    ],\n",
    "    stat(fft_aggregated, param=[\n",
    "        {'aggtype': s} \n",
    "        for s in ('centroid', 'variance', 'skew', 'kurtosis')\n",
    "    ]),\n",
    "    stat(fft_coefficient, param=[\n",
    "        {'coeff': k, 'attr': a}\n",
    "        for k, a in product(range(100), ('real', 'imag', 'abs', 'angle'))\n",
    "    ]),\n",
    "    *[stat(value_count, value=v) for v in (-1, 0, 1)],\n",
    "    *[stat(range_count, min=lo, max=hi) for lo, hi in [(-1, 1), (1e12, 0), (0, 1e12)]],\n",
    "    *[stat(approximate_entropy, m=2, r=r) for r in (.1, .3, .5, .7, .9)],\n",
    "    stat(friedrich_coefficients, param=[\n",
    "        {'coeff': coeff, 'm': 3, 'r': 30}\n",
    "        for coeff in range(4)\n",
    "    ]),\n",
    "    stat(max_langevin_fixed_point, m=3, r=30),\n",
    "    stat(linear_trend, param=[\n",
    "        {'attr': a} for a in ('pvalue', 'rvalue', 'intercept', 'slope', 'stderr')\n",
    "    ]),\n",
    "    stat(agg_linear_trend, param=[\n",
    "        {'attr': attr, 'chunk_len': i, 'f_agg': f}\n",
    "        for attr in ('rvalue', 'intercept', 'slope', 'stderr')\n",
    "        for i in (5, 10, 50)\n",
    "        for f in ('max', 'min', 'mean', 'var')\n",
    "    ]),\n",
    "    stat(augmented_dickey_fuller, param=[\n",
    "        {'attr': a} \n",
    "        for a in ('teststat', 'pvalue', 'usedlag')\n",
    "    ]),\n",
    "    *[stat(number_crossing_m, m=m) for m in (-1, 0, 1)],\n",
    "    stat(energy_ratio_by_chunks, param=[\n",
    "        {'num_segments': 10, 'segment_focus': i}\n",
    "        for i in range(10)\n",
    "    ]),\n",
    "    *[stat(ratio_beyond_r_sigma, r=r) for r in (0.5, 1, 1.5, 2, 2.5, 3, 5, 6, 7, 10)],\n",
    "    # stat(linear_trend_timewise, param=[\n",
    "    #     {'attr': a}\n",
    "    #     for a in ('pvalue', 'rvalue', 'intercept', 'slope', 'stderr')\n",
    "    # ])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs = tsfresh_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [StatsFeatures(funcs=funcs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending The Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn, x_tst = from_feather('x_trn', 'x_tst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/statsmodels/tsa/stattools.py:773: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  phi[1, 1] = sxx_m[1] / sxx_m[0]\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/tsfresh/feature_extraction/feature_calculators.py:989: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return y.dot(np.arange(len(y))**moment) / y.sum()\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:846: RuntimeWarning: divide by zero encountered in log\n",
      "  llf = -nobs2*np.log(2*np.pi) - nobs2*np.log(ssr / nobs) - nobs2\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/statsmodels/base/model.py:1100: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return self.params / self.bse\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/tsfresh/feature_extraction/feature_calculators.py:1880: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  res_data.append(np.sum(np.array_split(x, num_segments)[segment_focus] ** 2.0)/full_series_energy)\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7164"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, g = next(iter(x_trn.groupby('series_id')))\n",
    "len(generate_features_for_group(g, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction on train dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aaa4557d52c4073a4f334467f9522e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3810), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Feature extraction on train dataset')\n",
    "x_trn_rich = (\n",
    "    replace_not_numbers(\n",
    "        generate_features(\n",
    "            data=x_trn,\n",
    "            features=features, \n",
    "            ignore=ID_COLS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction on train dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e75f1858b634c60970bba8f136cf7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3816), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Feature extraction on train dataset')\n",
    "x_tst_rich = (\n",
    "    replace_not_numbers(\n",
    "        generate_features(\n",
    "            data=x_tst,\n",
    "            features=features, \n",
    "            ignore=ID_COLS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ck/data/careercon2019/tmp/trn_all.feather')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to_feather(x_trn_rich, 'trn_rich')\n",
    "to_feather(x_trn_rich, 'trn_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ck/data/careercon2019/tmp/tst_all.feather')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to_feather(x_tst_rich, 'tst_rich')\n",
    "to_feather(x_tst_rich, 'tst_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Fitting The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_trn, x_tst, y_trn = from_feather('trn_rich', 'tst_rich', 'y_trn')\n",
    "x_trn, x_tst, y_trn = from_feather('trn_all', 'tst_all', 'y_trn')\n",
    "enc = LabelEncoder()\n",
    "y_trn = pd.Series(enc.fit_transform(y_trn['surface']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance = calculate_relevance_table(x_trn, y_trn, ml_task='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_cols = relevance[relevance['relevant']].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn_rel = x_trn[rel_cols]\n",
    "x_tst_rel = x_tst[rel_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    x_trn_rel, y_trn, test_size=0.1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1/100\n",
      "\taccuracy improved: 86.61%\n",
      "Sample 3/100\n",
      "Sample 5/100\n",
      "\taccuracy improved: 90.29%\n",
      "Sample 9/100\n",
      "Sample 17/100\n",
      "Sample 33/100\n",
      "Sample 65/100\n",
      "Sample 100/100\n"
     ]
    }
   ],
   "source": [
    "n_iter = 100\n",
    "\n",
    "base = dict(num_iterations=100, seed=seed, objective='mutliclass')\n",
    "\n",
    "sampler = ParameterSampler({\n",
    "    'boosting': ['gbdt', 'dart'],\n",
    "    'colsamples_bytree': stats.uniform(0.3, 0.7),\n",
    "    'learning_rate': stats.uniform(0.005, 0.3),\n",
    "    'num_leaves': stats.randint(31, 500),\n",
    "    'min_data_in_leaf': stats.randint(20, 50),\n",
    "    'lambda_l1': stats.uniform(0.0, 0.001),\n",
    "    'lambda_l2': stats.uniform(0.0, 0.001),\n",
    "    'drop_rate': stats.uniform(0.05, 0.3)\n",
    "}, n_iter=n_iter)\n",
    "\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "expo = 1\n",
    "\n",
    "for i, params in enumerate(sampler):\n",
    "    if i % expo == 0 or i == (n_iter - 1):\n",
    "        print(f'Sample {i+1:d}/{n_iter:d}')\n",
    "        expo *= 2\n",
    "    model = lgb.LGBMClassifier(**base, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat = model.predict(X_valid)\n",
    "    acc = (y_hat == y_valid).mean()\n",
    "    if acc > best_acc:\n",
    "        print(f'\\taccuracy improved: {acc:2.2%}')\n",
    "        best_acc = acc\n",
    "        best_params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting': 'gbdt',\n",
       " 'colsamples_bytree': 0.8724378755450763,\n",
       " 'drop_rate': 0.24689475810489686,\n",
       " 'lambda_l1': 1.1280314202778308e-05,\n",
       " 'lambda_l2': 0.0007353536939500786,\n",
       " 'learning_rate': 0.18604839329048833,\n",
       " 'min_data_in_leaf': 46,\n",
       " 'num_leaves': 447}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = lgb.LGBMClassifier(\n",
    "#     boosting='rf', bagging_freq=1, bagging_fraction=0.66,\n",
    "#     n_estimators=1000, learning_rate=0.005,\n",
    "#     colsample_bytree=0.3, objective='multiclass',\n",
    "#     metric='None', num_leaves=200, num_class=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = lgb.LGBMClassifier(num_iterations=10000, seed=seed, metric='None', **best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\tvalid_0's accuracy: 0.863517\n",
      "[200]\tvalid_0's accuracy: 0.87664\n",
      "[300]\tvalid_0's accuracy: 0.884514\n",
      "[400]\tvalid_0's accuracy: 0.889764\n",
      "[500]\tvalid_0's accuracy: 0.892388\n",
      "[600]\tvalid_0's accuracy: 0.892388\n",
      "[700]\tvalid_0's accuracy: 0.892388\n",
      "[800]\tvalid_0's accuracy: 0.892388\n",
      "[900]\tvalid_0's accuracy: 0.895013\n",
      "[1000]\tvalid_0's accuracy: 0.895013\n",
      "[1100]\tvalid_0's accuracy: 0.889764\n",
      "[1200]\tvalid_0's accuracy: 0.889764\n",
      "[1300]\tvalid_0's accuracy: 0.889764\n",
      "[1400]\tvalid_0's accuracy: 0.887139\n",
      "Early stopping, best iteration is:\n",
      "[423]\tvalid_0's accuracy: 0.895013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.3,\n",
       "        importance_type='split', learning_rate=0.05, max_depth=-1,\n",
       "        metric='None', min_child_samples=20, min_child_weight=0.001,\n",
       "        min_split_gain=0.0, n_estimators=10000, n_jobs=-1, num_class=9,\n",
       "        num_leaves=100, objective='multiclass', random_state=None,\n",
       "        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(\n",
    "#     X_train, y_train, \n",
    "#     eval_set=[(X_valid, y_valid)], \n",
    "#     eval_metric=accuracy,\n",
    "#     early_stopping_rounds=1000,\n",
    "#     verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = None\n",
    "# assert n is not None, 'Need to set number of iterations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = lgb.LGBMClassifier(num_iterations=n, seed=seed, metric='None', **best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting='gbdt', boosting_type='gbdt', class_weight=None,\n",
       "        colsample_bytree=1.0, colsamples_bytree=0.8724378755450763,\n",
       "        drop_rate=0.24689475810489686, importance_type='split',\n",
       "        lambda_l1=1.1280314202778308e-05, lambda_l2=0.0007353536939500786,\n",
       "        learning_rate=0.18604839329048833, max_depth=-1, metric='None',\n",
       "        min_child_samples=20, min_child_weight=0.001, min_data_in_leaf=46,\n",
       "        min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n",
       "        num_iterations=400, num_leaves=447, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, seed=1,\n",
       "        silent=True, subsample=1.0, subsample_for_bin=200000,\n",
       "        subsample_freq=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(x_trn_rel, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(\n",
    "    boosting='dart', bagging_freq=1, bagging_fraction=0.66,\n",
    "    n_estimators=10000, learning_rate=0.005,\n",
    "    colsample_bytree=0.3, objective='multiclass',\n",
    "    metric='None', num_leaves=200, num_class=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's accuracy: 0.800525\n",
      "[200]\tvalid_0's accuracy: 0.813648\n",
      "[300]\tvalid_0's accuracy: 0.816273\n",
      "[400]\tvalid_0's accuracy: 0.818898\n",
      "[500]\tvalid_0's accuracy: 0.834646\n",
      "[600]\tvalid_0's accuracy: 0.83727\n",
      "[700]\tvalid_0's accuracy: 0.83727\n",
      "[800]\tvalid_0's accuracy: 0.84252\n",
      "[900]\tvalid_0's accuracy: 0.845144\n",
      "[1000]\tvalid_0's accuracy: 0.847769\n",
      "[1100]\tvalid_0's accuracy: 0.847769\n",
      "[1200]\tvalid_0's accuracy: 0.853018\n",
      "[1300]\tvalid_0's accuracy: 0.855643\n",
      "[1400]\tvalid_0's accuracy: 0.858268\n",
      "[1500]\tvalid_0's accuracy: 0.855643\n",
      "[1600]\tvalid_0's accuracy: 0.858268\n",
      "[1700]\tvalid_0's accuracy: 0.858268\n",
      "[1800]\tvalid_0's accuracy: 0.858268\n",
      "[1900]\tvalid_0's accuracy: 0.858268\n",
      "[2000]\tvalid_0's accuracy: 0.858268\n",
      "[2100]\tvalid_0's accuracy: 0.860892\n",
      "[2200]\tvalid_0's accuracy: 0.860892\n",
      "[2300]\tvalid_0's accuracy: 0.860892\n",
      "[2400]\tvalid_0's accuracy: 0.868766\n",
      "[2500]\tvalid_0's accuracy: 0.868766\n",
      "[2600]\tvalid_0's accuracy: 0.863517\n",
      "[2700]\tvalid_0's accuracy: 0.863517\n",
      "[2800]\tvalid_0's accuracy: 0.863517\n",
      "[2900]\tvalid_0's accuracy: 0.866142\n",
      "[3000]\tvalid_0's accuracy: 0.866142\n",
      "[3100]\tvalid_0's accuracy: 0.863517\n",
      "[3200]\tvalid_0's accuracy: 0.863517\n",
      "[3300]\tvalid_0's accuracy: 0.866142\n",
      "[3400]\tvalid_0's accuracy: 0.868766\n",
      "[3500]\tvalid_0's accuracy: 0.866142\n",
      "[3600]\tvalid_0's accuracy: 0.866142\n",
      "[3700]\tvalid_0's accuracy: 0.868766\n",
      "[3800]\tvalid_0's accuracy: 0.868766\n",
      "[3900]\tvalid_0's accuracy: 0.868766\n",
      "[4000]\tvalid_0's accuracy: 0.868766\n",
      "[4100]\tvalid_0's accuracy: 0.871391\n",
      "[4200]\tvalid_0's accuracy: 0.871391\n",
      "[4300]\tvalid_0's accuracy: 0.868766\n",
      "[4400]\tvalid_0's accuracy: 0.868766\n",
      "[4500]\tvalid_0's accuracy: 0.868766\n",
      "[4600]\tvalid_0's accuracy: 0.868766\n",
      "[4700]\tvalid_0's accuracy: 0.868766\n",
      "[4800]\tvalid_0's accuracy: 0.871391\n",
      "[4900]\tvalid_0's accuracy: 0.871391\n",
      "[5000]\tvalid_0's accuracy: 0.871391\n",
      "[5100]\tvalid_0's accuracy: 0.871391\n",
      "[5200]\tvalid_0's accuracy: 0.874016\n",
      "[5300]\tvalid_0's accuracy: 0.874016\n",
      "[5400]\tvalid_0's accuracy: 0.874016\n",
      "[5500]\tvalid_0's accuracy: 0.874016\n",
      "[5600]\tvalid_0's accuracy: 0.874016\n",
      "[5700]\tvalid_0's accuracy: 0.87664\n",
      "[5800]\tvalid_0's accuracy: 0.87664\n",
      "[5900]\tvalid_0's accuracy: 0.88189\n",
      "[6000]\tvalid_0's accuracy: 0.879265\n",
      "[6100]\tvalid_0's accuracy: 0.879265\n",
      "[6200]\tvalid_0's accuracy: 0.88189\n",
      "[6300]\tvalid_0's accuracy: 0.88189\n",
      "[6400]\tvalid_0's accuracy: 0.88189\n",
      "[6500]\tvalid_0's accuracy: 0.884514\n",
      "[6600]\tvalid_0's accuracy: 0.884514\n",
      "[6700]\tvalid_0's accuracy: 0.884514\n",
      "[6800]\tvalid_0's accuracy: 0.884514\n",
      "[6900]\tvalid_0's accuracy: 0.884514\n",
      "[7000]\tvalid_0's accuracy: 0.884514\n",
      "[7100]\tvalid_0's accuracy: 0.884514\n",
      "[7200]\tvalid_0's accuracy: 0.884514\n",
      "[7300]\tvalid_0's accuracy: 0.88189\n",
      "[7400]\tvalid_0's accuracy: 0.88189\n",
      "[7500]\tvalid_0's accuracy: 0.88189\n",
      "[7600]\tvalid_0's accuracy: 0.88189\n",
      "[7700]\tvalid_0's accuracy: 0.88189\n",
      "[7800]\tvalid_0's accuracy: 0.88189\n",
      "[7900]\tvalid_0's accuracy: 0.88189\n",
      "[8000]\tvalid_0's accuracy: 0.88189\n",
      "[8100]\tvalid_0's accuracy: 0.88189\n",
      "[8200]\tvalid_0's accuracy: 0.88189\n",
      "[8300]\tvalid_0's accuracy: 0.88189\n",
      "[8400]\tvalid_0's accuracy: 0.88189\n",
      "[8500]\tvalid_0's accuracy: 0.88189\n",
      "[8600]\tvalid_0's accuracy: 0.887139\n",
      "[8700]\tvalid_0's accuracy: 0.887139\n",
      "[8800]\tvalid_0's accuracy: 0.889764\n",
      "[8900]\tvalid_0's accuracy: 0.889764\n",
      "[9000]\tvalid_0's accuracy: 0.887139\n",
      "[9100]\tvalid_0's accuracy: 0.887139\n",
      "[9200]\tvalid_0's accuracy: 0.887139\n",
      "[9300]\tvalid_0's accuracy: 0.887139\n",
      "[9400]\tvalid_0's accuracy: 0.887139\n",
      "[9500]\tvalid_0's accuracy: 0.887139\n",
      "[9600]\tvalid_0's accuracy: 0.887139\n",
      "[9700]\tvalid_0's accuracy: 0.887139\n",
      "[9800]\tvalid_0's accuracy: 0.887139\n",
      "[9900]\tvalid_0's accuracy: 0.887139\n",
      "[10000]\tvalid_0's accuracy: 0.887139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.66, bagging_freq=1, boosting='dart',\n",
       "        boosting_type='gbdt', class_weight=None, colsample_bytree=0.3,\n",
       "        importance_type='split', learning_rate=0.005, max_depth=-1,\n",
       "        metric='None', min_child_samples=20, min_child_weight=0.001,\n",
       "        min_split_gain=0.0, n_estimators=10000, n_jobs=-1, num_class=9,\n",
       "        num_leaves=200, objective='multiclass', random_state=None,\n",
       "        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          eval_metric=accuracy,\n",
    "          verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.predict(x_tst_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 52.3k/52.3k [00:00<00:00, 45.9kB/s]\n",
      "Successfully submitted to CareerCon 2019 - Help Navigate Robots "
     ]
    }
   ],
   "source": [
    "submit = pd.read_csv(SAMPLE)\n",
    "submit['surface'] = enc.inverse_transform(test)\n",
    "submit.to_csv('submit.csv', index=None)\n",
    "!kaggle c submit career-con-2019 -f 'submit.csv' -m \"10000 trees\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
